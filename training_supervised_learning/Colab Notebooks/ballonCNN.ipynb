{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import torch \n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from typing import List, Tuple\n",
    "from matplotlib import animation\n",
    "\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "from gym.wrappers import FlattenObservation\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class balloon3D:\n",
    "    def __init__(self,width,length,height):\n",
    "        # self.obs_size = obs_size\n",
    "        self.x, self.y , self.z = 0,0,0\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.length = length\n",
    "        self.map = None\n",
    "\n",
    "    def reset(self,start_x,start_y,start_z):\n",
    "        self.x, self.y ,self.z = start_x,start_y,start_z\n",
    "\n",
    "    def up(self):\n",
    "        self.z = min(self.height-1,self.z+1)\n",
    "\n",
    "    def down(self):\n",
    "        self.z = max(self.z-1,0)\n",
    "\n",
    "    def step(self):\n",
    "        newx = int(max(0,min(self.x + self.map[0,self.x,self.y,self.z],self.width-1)))\n",
    "        newy = int(max(0,min(self.y + self.map[1,self.x,self.y,self.z],self.length-1)))\n",
    "        self.x = newx\n",
    "        self.y = newy\n",
    "\n",
    "    def generate_map(self,eta,mu):\n",
    "        self.map = np.zeros((2,self.width,self.length,self.height))\n",
    "        for i in range(self.width):\n",
    "            for j in  range(self.length):\n",
    "                for k in range(self.height):\n",
    "                    for l in range(2):\n",
    "                        r = np.random.random()\n",
    "                        if(r>eta):\n",
    "                            r= np.random.random()\n",
    "                            if(r>mu):\n",
    "                                self.map[l,i,j,k] = 1\n",
    "                            else:\n",
    "                                self.map[l,i,j,k] = -1\n",
    "                            if j == self.length-1:\n",
    "                                self.map[1,i,j,k] = -1\n",
    "                            if j == 0:\n",
    "                                self.map[1,i,j,k] = 1\n",
    "                            if i == self.width-1:\n",
    "                                self.map[0,i,j,k] = -1\n",
    "                            if i == 0:\n",
    "                                self.map[0,i,j,k] = 1\n",
    "\n",
    "\n",
    "    def set_map(self,map):\n",
    "        self.map = map\n",
    "\n",
    "    def get_winds(self):\n",
    "        # \"\"\"r = self.obs_size\n",
    "        # self.obs = np.zeros((2*r+1,2*r+1,self.height,2))\n",
    "        # for i in range(self.x - r , self.x + r + 1):\n",
    "        #     for j in range(self.y - r, self.y + r + 1):\n",
    "        #         for k in range(0,self.height):\n",
    "        #             for l in range(2):\n",
    "        #                 self.obs[i - self.x + r, j- self.y + r,k,l]  = (self.map[i%self.width,j%self.length,k,l])\n",
    "        # return self.obs\"\"\"\n",
    "        self.obs= self.map\n",
    "        return self.obs\n",
    "\n",
    "    \n",
    "    def render_obs(self):\n",
    "        # r =self.obs_size*2+1\n",
    "        for z in range(self.height):\n",
    "            for j in range(self.length):\n",
    "                s = ''\n",
    "                for i in range(self.width):\n",
    "                    l = ''\n",
    "                    if(self.obs[0,i,j,z] == 0):\n",
    "                        l += '.'\n",
    "                    elif(self.obs[0,i,j,z] == 1):\n",
    "                        l+='>'\n",
    "                    elif(self.obs[0,i,j,z] == -1):\n",
    "                        l+='<'\n",
    "\n",
    "                    if(self.obs[1,i,j,z] == 0):\n",
    "                        l += '.'\n",
    "                    elif(self.obs[1,i,j,z] == 1):\n",
    "                        l+='v'\n",
    "                    elif(self.obs[1,i,j,z] == -1):\n",
    "                        l+='^'                  \n",
    "                    s+= l+ \" \"\n",
    "                print(s)\n",
    "            print(\" \")\n",
    "    \n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        for z in range(self.height):\n",
    "            for j in range(self.length):\n",
    "                s = ''\n",
    "                for i in range(self.width):\n",
    "                    if self.x == i and self.y == j and self.z == z : \n",
    "                        s+= 'OO '\n",
    "                    else:\n",
    "                        l = ''\n",
    "                        if(self.map[0,i,j,z] == 0):\n",
    "                            l += '.'\n",
    "                        elif(self.map[0,i,j,z] == 1):\n",
    "                            l+='>'\n",
    "                        elif(self.map[0,i,j,z] == -1):\n",
    "                            l+='<'\n",
    "\n",
    "                        if(self.map[1,i,j,z] == 0):\n",
    "                            l += '.'\n",
    "                        elif(self.map[1,i,j,z] == 1):\n",
    "                            l+='v'\n",
    "                        elif(self.map[1,i,j,z] == -1):\n",
    "                            l+='^'\n",
    "                        \n",
    "                        s+= l+ \" \"\n",
    "                print(s)\n",
    "            print(\" \")\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OO >v <v >v <v >v <v .v .v <v \n",
      ">. .v .. .^ >^ .. .. >. <. <. \n",
      ">. <^ <. <. <v .. >^ .v .v .. \n",
      ">v .. >. >^ <v <v .v >v .^ <. \n",
      ">. <. <. .. >. .^ >. .v .^ .. \n",
      ">^ .. <^ <. <. .^ >v >v >. <v \n",
      ">^ >^ .v >^ >. >^ .. .v <. <v \n",
      ">v .^ .v <. .. <. .. <v .. <^ \n",
      ">. .. <v >v .v .^ .v .. <^ <^ \n",
      ".. <^ >^ <^ >^ >^ .^ .. .. <^ \n",
      " \n",
      ">v >v <v .v >v <v .v <v .v <v \n",
      ">^ <. .v >. >v <v .v .. .^ <^ \n",
      ">^ .^ <v >^ >^ .^ .. .. <^ <v \n",
      ">^ .. <^ <. <. >. .. >. <^ <. \n",
      ">^ .^ >^ .. >v >v .^ <. >. <^ \n",
      ".. >. >^ <^ <^ <^ >^ <^ .. <^ \n",
      ">. .. .^ <. .v <. .^ >v >. <. \n",
      ">. >v >. .v >^ .. >v >^ .v <. \n",
      ">v <. .v >v >^ >v .. .. <. <^ \n",
      ">^ .. >^ <^ <^ >^ .. .. >^ <^ \n",
      " \n",
      ">v >v .. >v >v .v <v <v >v <v \n",
      ">. .^ .. <^ .^ .^ <^ .^ <. <. \n",
      ">^ >. <. >v .v <v >v >. >v <. \n",
      ">. .^ <. <. <v .v .. <. <^ <v \n",
      ".. >. .. .v .^ >v >^ <. >v <. \n",
      ">. >. .. .^ <^ >^ <^ <. >^ <^ \n",
      ">v <. .^ .. <^ <^ <. <^ >^ <v \n",
      ">^ <v .^ >^ <^ .. .. >^ >^ <^ \n",
      ">^ .^ .. >^ >. .^ .. .^ .^ <v \n",
      ">^ <^ >^ .. >^ .. <^ <^ .. <^ \n",
      " \n"
     ]
    }
   ],
   "source": [
    "bal = balloon3D(10,10,3)\n",
    "bal.generate_map(0.4,0.5)\n",
    "bal.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. >v <v >v <v >v <v .v .v <v \n",
      ">. .v .. .^ >^ .. .. >. <. <. \n",
      ">. <^ <. <. <v .. >^ .v .v .. \n",
      ">v .. >. >^ <v <v .v >v .^ <. \n",
      ">. <. <. .. >. .^ >. .v .^ .. \n",
      ">^ .. <^ <. <. .^ >v >v >. <v \n",
      ">^ >^ .v >^ >. >^ .. .v <. <v \n",
      ">v .^ .v <. .. <. .. <v .. <^ \n",
      ">. .. <v >v .v .^ .v .. <^ <^ \n",
      ".. <^ >^ <^ >^ >^ .^ .. .. <^ \n",
      " \n",
      ">v >v <v .v >v <v .v <v .v <v \n",
      ">^ <. .v >. >v <v .v .. .^ <^ \n",
      ">^ .^ <v >^ >^ .^ .. .. <^ <v \n",
      ">^ OO <^ <. <. >. .. >. <^ <. \n",
      ">^ .^ >^ .. >v >v .^ <. >. <^ \n",
      ".. >. >^ <^ <^ <^ >^ <^ .. <^ \n",
      ">. .. .^ <. .v <. .^ >v >. <. \n",
      ">. >v >. .v >^ .. >v >^ .v <. \n",
      ">v <. .v >v >^ >v .. .. <. <^ \n",
      ">^ .. >^ <^ <^ >^ .. .. >^ <^ \n",
      " \n",
      ">v >v .. >v >v .v <v <v >v <v \n",
      ">. .^ .. <^ .^ .^ <^ .^ <. <. \n",
      ">^ >. <. >v .v <v >v >. >v <. \n",
      ">. .^ <. <. <v .v .. <. <^ <v \n",
      ".. >. .. .v .^ >v >^ <. >v <. \n",
      ">. >. .. .^ <^ >^ <^ <. >^ <^ \n",
      ">v <. .^ .. <^ <^ <. <^ >^ <v \n",
      ">^ <v .^ >^ <^ .. .. >^ >^ <^ \n",
      ">^ .^ .. >^ >. .^ .. .^ .^ <v \n",
      ">^ <^ >^ .. >^ .. <^ <^ .. <^ \n",
      " \n"
     ]
    }
   ],
   "source": [
    "bal.step()\n",
    "bal.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. >v <v >v <v >v <v .v .v <v \n",
      ">. .v .. .^ >^ .. .. >. <. <. \n",
      ">. <^ <. <. <v .. >^ .v .v .. \n",
      ">v .. >. >^ <v <v .v >v .^ <. \n",
      ">. <. <. .. >. .^ >. .v .^ .. \n",
      ">^ .. <^ <. <. .^ >v >v >. <v \n",
      ">^ >^ .v >^ >. >^ .. .v <. <v \n",
      ">v .^ .v <. .. <. .. <v .. <^ \n",
      ">. .. <v >v .v .^ .v .. <^ <^ \n",
      ".. <^ >^ <^ >^ >^ .^ .. .. <^ \n",
      " \n",
      ">v >v <v .v >v <v .v <v .v <v \n",
      ">^ <. .v >. >v <v .v .. .^ <^ \n",
      ">^ .^ <v >^ >^ .^ .. .. <^ <v \n",
      ">^ .. <^ <. <. >. .. >. <^ <. \n",
      ">^ .^ >^ .. >v >v .^ <. >. <^ \n",
      ".. >. >^ <^ <^ <^ >^ <^ .. <^ \n",
      ">. .. .^ <. .v <. .^ >v >. <. \n",
      ">. >v >. .v >^ .. >v >^ .v <. \n",
      ">v <. .v >v >^ >v .. .. <. <^ \n",
      ">^ .. >^ <^ <^ >^ .. .. >^ <^ \n",
      " \n",
      ">v >v .. >v >v .v <v <v >v <v \n",
      ">. .^ .. <^ .^ .^ <^ .^ <. <. \n",
      ">^ >. <. >v .v <v >v >. >v <. \n",
      ">. .^ <. <. <v .v .. <. <^ <v \n",
      ".. >. .. .v .^ >v >^ <. >v <. \n",
      ">. >. .. .^ <^ >^ <^ <. >^ <^ \n",
      ">v <. .^ .. <^ <^ <. <^ >^ <v \n",
      ">^ <v .^ >^ <^ .. .. >^ >^ <^ \n",
      ">^ .^ .. >^ >. .^ .. .^ .^ <v \n",
      ">^ <^ >^ .. >^ .. <^ <^ .. <^ \n",
      " \n"
     ]
    }
   ],
   "source": [
    "bal.get_winds()\n",
    "bal.render_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalEnv(gym.Env):\n",
    "    def __init__(self, render_mode=None, s_x = 16 , s_y =16 , s_z = 4):\n",
    "        self.balloon = balloon3D(s_x,s_y,s_z)\n",
    "\n",
    "        self._target_location = [s_x-1,s_y-1,s_z-1]\n",
    "\n",
    "        self.pos = np.zeros((1,s_x,s_y,s_z))\n",
    "\n",
    "        self.observation_space = spaces.Box(-1, 1, shape=(3,s_x,s_y,s_z), dtype=int)\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def reset(self):\n",
    "        self.evo_time = 100\n",
    "\n",
    "        self.reward = 0\n",
    "\n",
    "        self.reward_x, self.reward_y = 0,0\n",
    "\n",
    "        self.balloon.generate_map(0.35,0.5)\n",
    "\n",
    "        self.time = 0\n",
    "\n",
    "        self.balloon.reset(0,0,0)\n",
    "\n",
    "        self._agent_location = [0,0,0]\n",
    "\n",
    "        self.pos[0,self._agent_location[0],self._agent_location[1],self._agent_location[2]] = 10\n",
    "\n",
    "        self.pos[0,self._target_location[0],self._target_location[1],self._target_location[2]] = -10\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.time += 1\n",
    "\n",
    "        self.pos[0,self._agent_location[0],self._agent_location[1],self._agent_location[2]] = 0\n",
    "\n",
    "        if action == 0:\n",
    "            self.balloon.step()\n",
    "        if action == 1:\n",
    "            self.balloon.up()\n",
    "        if action == 2:\n",
    "            self.balloon.down()\n",
    "\n",
    "\n",
    "        \n",
    "        self._agent_location = np.array([self.balloon.x , self.balloon.y,self.balloon.z])\n",
    "\n",
    "        self.pos[0,self._agent_location[0],self._agent_location[1],self._agent_location[2]] = 10\n",
    "\n",
    "        terminated  = ((self._agent_location[0]==self._target_location[0])and(self._agent_location[1]==self._target_location[1]))\n",
    "        \n",
    "        distx = (self._target_location[0] - self._agent_location[0])\n",
    "        disty = self._agent_location[1] - self._target_location[1]\n",
    "\n",
    "\n",
    "\n",
    "        self.reward = -(np.sqrt((distx)**2 + (disty)**2))\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation, self.reward, terminated, self.time ==self.evo_time, {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "\n",
    "        winds =  self.balloon.get_winds()\n",
    "        obs = np.concatenate((self.pos, winds), axis=0)\n",
    "        return  obs\n",
    "\n",
    "    def render(self):\n",
    "        self.balloon.render()\n",
    "        \n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16, 16, 4])\n"
     ]
    }
   ],
   "source": [
    "env = BalEnv()\n",
    "env.reset()\n",
    "obs = torch.tensor([env._get_obs()])\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      ">v >v <v <v <v .v .. >v >v .v <v >v .v <v <v <v \n",
      ".. OO .v <. <. >^ <. >^ .^ <v .^ .^ <v <. <v <. \n",
      ">^ <^ <v .. >v .v >^ <v >^ >v >v >v <. <. >. <. \n",
      ">v >v <v <v <v <. <. >v .^ <. <^ >^ >^ <. >^ <^ \n",
      ">^ <^ <. <. .^ <^ >^ <. <v .. <v .v .^ >v <. <v \n",
      ">. >v .. <v .^ .^ <v >^ .v .. <v <^ <v >v <^ <^ \n",
      ">v <^ .^ >. >v .^ >^ >^ >. >^ >^ >v .. >^ <v <v \n",
      ">v <v >^ .. <v .. <. <^ <. .. .. >v <v >v <. <^ \n",
      ">v .v >. .^ <. .v <. .^ .^ <v <. >^ >^ <. >^ <v \n",
      ">. .^ >. .. <v .^ <v >. >. .v >. .v <. <^ <v .. \n",
      ">v >. <^ .v <v .v .. >. .v .. >^ <. >. .. .v <^ \n",
      ">^ .v .v <. <v .v <v <v >. >^ <. >. <v <. <. <v \n",
      ">. >^ <v <^ >^ >v .. .. >. .v <. <. .^ >v .^ <v \n",
      ">. <^ <v <v .^ .. .. >v .v <. >^ <^ .^ <^ .^ <. \n",
      ".. <. <v .^ <. <. <^ .v >^ <^ >v <. .. .. .v <v \n",
      ".. .^ .. <^ .^ .. <^ .^ .. <^ >^ >^ .^ <^ <^ <^ \n",
      " \n",
      ">v >v .v >v >v <v .v <v .v >v >v >v .v <v <v <v \n",
      ">v <v .^ .. >. .. >. >^ .^ <v <^ <. >^ .. .^ <^ \n",
      ">^ <v >v <^ >v >v .. .v >. >v >. <v .v .. >v <^ \n",
      ">^ <^ >v .v >. >. .^ .. .^ >^ .. >^ .^ >^ >v <. \n",
      ".. <^ <v >^ .v <. >. >. .. >. .^ >. <^ <v .^ <v \n",
      ">v <. >. >. >. .^ .v >. .v .v .. .^ .. >^ <. <^ \n",
      ">^ <. <v <^ <^ >. .v .^ .^ <^ <^ >v <. .. .^ <^ \n",
      ">v >^ .^ <. >. <v >v .v <v .^ <. <v <^ <^ .^ <^ \n",
      ">v >^ .^ .. .^ .^ >v .^ >. .^ .^ >v .^ .^ >^ <v \n",
      ">^ .. >v >^ >^ .. >v >v >v .. .. <v .. .. <. <v \n",
      ">. <v <. >v <. .. >^ .^ .^ .^ >v <v .^ <v >^ <v \n",
      ">. .. <v .. <^ <^ .. >v .^ .. >. >. <v >^ <v <^ \n",
      ">v <v .. .v .. >v >^ <v <. .. >. .v .^ <. .^ <. \n",
      ">v >v <^ .. <^ .. >. >v <^ <. <v <^ >v >. >. <v \n",
      ".. .^ .. .. <v .. >^ <^ .. >^ >^ >. .. <. >. <. \n",
      ">^ <^ <^ .. >^ .^ <^ .. <^ <^ .^ .^ .^ <^ <^ <^ \n",
      " \n",
      ".. <v >v .v >v <v <v <v <v .v <v <v >v >v .v .. \n",
      ".. .. .. <^ >^ <^ .. <^ >^ <. .v <. .. .v <v <^ \n",
      ">^ <^ >. .. .. <. <v <. <. <^ <. .^ <. <. .. <. \n",
      ">. <v >v >^ <. <v <. >v <. >. >. <v <v <v >v .. \n",
      ">. .v .. .^ >^ <. .v .^ >v <. >. .^ .. .^ >v <^ \n",
      ">. .^ >v <v .v .v .^ .v .v .. >. <. .. >^ >^ <. \n",
      ".. .v <v <v .. >. <v .. >. >^ >v .^ <v .v .v <^ \n",
      ">v >v >. <^ .^ <v >. >. >^ <^ .v <v .^ <^ >v <^ \n",
      ">v .. >. .. <v .^ .. .. .^ <. <. <. <v >v >. <^ \n",
      ">v <^ >v <^ >^ .v <. >v .. .^ .. <. .^ .. >. .. \n",
      ">. .v >^ <^ <. .v >. >^ >v <. >v >^ <^ .. .^ <^ \n",
      ">v <v <. .v <^ >v <^ .^ >v <v <v <v .^ <. .^ <^ \n",
      ">v .. .v .^ .. .. .^ >. >v <v <. >^ <. >v .. <^ \n",
      ">. <. <. .. .^ .. >. .^ <. .^ >v <^ >^ >v >^ <^ \n",
      ".. <. <^ >^ <. <^ .v .^ <. <^ .v <. <. <v >^ .. \n",
      ">^ >^ >^ <^ >^ <^ <^ .^ .^ >^ .^ .^ .. >^ <^ <^ \n",
      " \n",
      ">v .v .v .. <v <v .v <v >v .v .. .. <v >v >v .. \n",
      ">^ .. .^ .^ <v .v <^ .^ <^ .^ >^ >v >. >. >v <^ \n",
      ">v .. .v <v >v .^ .v .v .. <^ .^ <. .^ .^ .^ <^ \n",
      ">v >. >^ >v .. >. .^ <. <^ <. <. >. <^ >v >^ <^ \n",
      ">v .^ >^ >. >. .. >^ .. <v >^ .. <^ .. >^ >^ <. \n",
      ">v .^ >. <v .v >. .. >v >^ .. >^ .^ >. .v >v <v \n",
      ">v .v .v <^ .v >. .. <. .^ <^ >. .v >^ <v .v <. \n",
      ">v .. >v <. .v .v .. <. >. .v >. <. .^ <v >. .. \n",
      ">v >. <^ >^ <. <. >^ .. .. >^ <^ >v .v >. <v .. \n",
      ">v >. .. <v .^ .. <v >^ <v .^ >. .^ .. .. .v <. \n",
      ">. .^ <. >. <^ .. <. <^ .v >^ <. >. .v .. .^ <^ \n",
      ">v >. <v .v >^ .v .v <^ >v <^ .v >^ <. .. >^ <v \n",
      ">v <v >v .^ .v <^ <^ .^ .^ .^ .^ >^ <. >v <. <v \n",
      ">^ .. >v .. .v <^ .^ <^ >v <v >. >^ .. .. .. <v \n",
      ">v >^ <^ <^ <. >. .v <. >^ <. .. >^ >v .. >. <. \n",
      ">^ .^ <^ <^ >^ >^ <^ .. <^ >^ <^ <^ .^ <^ >^ <^ \n",
      " \n",
      "-19.79898987322333\n"
     ]
    }
   ],
   "source": [
    "a = env.action_space.sample()\n",
    "print(a)\n",
    "_,r,_,_,_ = env.step(a)\n",
    "env.render()\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple3DCNN, self).__init__()\n",
    "        self.soft = torch.nn.Softmax(dim = -1)\n",
    "        self.conv1 = nn.Conv3d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n",
    "        self.adap_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        # Assuming input volume is 32x32x32, after two pooling layers it becomes 8x8x8\n",
    "        self.fc1 = nn.Linear(128, 512)\n",
    "        self.fc2 = nn.Linear(512, 3)  # Assuming binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.adap_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x =  F.relu(self.fc1(x))\n",
    "        x =  F.relu(self.fc2(x))\n",
    "        return self.soft(x)\n",
    "\n",
    "class QNetwork:\n",
    "    def __init__(self, env, lr, logdir=None):\n",
    "        # Define Q-network with specified architecture\n",
    "        self.net = Simple3DCNN()\n",
    "        self.env = env\n",
    "        self.lr = lr \n",
    "        self.logdir = logdir\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "\n",
    "    def load_model(self, model_file):\n",
    "        # Load pre-trained model from a file\n",
    "        return self.net.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    def load_model_weights(self, weight_file):\n",
    "        # Load pre-trained model weights from a file\n",
    "        return self.net.load_state_dict(torch.load(weight_file))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16, 16, 4])\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.double).unsqueeze(0)\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, env, memory_size=50000, burn_in=10000):\n",
    "        # Initializes the replay memory, which stores transitions recorded from the agent taking actions in the environment.\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.memory = collections.deque([], maxlen=memory_size)\n",
    "        self.env = env\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        # Returns a batch of randomly sampled transitions to be used for training the model.\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def append(self, transition):\n",
    "        # Appends a transition to the replay memory.\n",
    "        self.memory.append(transition)\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class DQN_Agent:\n",
    "\n",
    "    def __init__(self, env, lr=5e-4, render=False):\n",
    "        # Initialize the DQN Agent.\n",
    "        self.env = env\n",
    "        self.lr = lr\n",
    "        self.policy_net = QNetwork(self.env, self.lr)\n",
    "        self.target_net = QNetwork(self.env, self.lr)\n",
    "        self.target_net.net.load_state_dict(self.policy_net.net.state_dict())  # Copy the weight of the policy network\n",
    "        self.rm = ReplayMemory(self.env)\n",
    "        self.burn_in_memory()\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.c = 0\n",
    "\n",
    "    def burn_in_memory(self):\n",
    "        # Initialize replay memory with a burn-in number of episodes/transitions.\n",
    "        cnt = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        state, _ = self.env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.double).unsqueeze(0)\n",
    "\n",
    "        # Iterate until we store \"burn_in\" buffer\n",
    "        while cnt < self.rm.burn_in:\n",
    "            # Reset environment if terminated or truncated\n",
    "            if terminated or truncated:\n",
    "                state, _ = self.env.reset()\n",
    "                state = torch.tensor(state, dtype=torch.double).unsqueeze(0)\n",
    "            \n",
    "            # Randomly select an action (left or right) and take a step\n",
    "            action = torch.tensor(self.env.action_space.sample()).reshape(1, 1)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "            reward = torch.tensor([reward])\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.double).unsqueeze(0)\n",
    "                \n",
    "            # Store new experience into memory\n",
    "            transition = Transition(state, action, next_state, reward)\n",
    "            self.rm.memory.append(transition)\n",
    "            state = next_state\n",
    "            cnt += 1\n",
    "\n",
    "    def epsilon_greedy_policy(self, q_values, epsilon=0.05):\n",
    "        # Implement an epsilon-greedy policy. \n",
    "        p = random.random()\n",
    "        if p > epsilon:\n",
    "            with torch.no_grad():\n",
    "                return self.greedy_policy(q_values)\n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)\n",
    "\n",
    "    def greedy_policy(self, q_values):\n",
    "        # Implement a greedy policy for test time.\n",
    "        return torch.argmax(q_values)\n",
    "        \n",
    "    def train(self):\n",
    "        # Train the Q-network using Deep Q-learning.\n",
    "        state, _ = self.env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.double).unsqueeze(0)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Loop until reaching the termination state\n",
    "        while not (terminated or truncated):\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net.net(state)\n",
    "\n",
    "            # Decide the next action with epsilon greedy strategy\n",
    "            action = self.epsilon_greedy_policy(q_values).reshape(1, 1)\n",
    "            \n",
    "            # Take action and observe reward and next state\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "            reward = torch.tensor([reward])\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.double).unsqueeze(0)\n",
    "\n",
    "            # Store the new experience\n",
    "            transition = Transition(state, action, next_state, reward)\n",
    "            self.rm.memory.append(transition)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Sample minibatch with size N from memory\n",
    "            transitions = self.rm.sample_batch(self.batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "            # Get current and next state values\n",
    "            state_action_values = self.policy_net.net(state_batch).gather(1, action_batch) # extract values corresponding to the actions Q(S_t, A_t)\n",
    "            next_state_values = torch.zeros(self.batch_size)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # no next_state_value update if an episode is terminated (next_satate = None)\n",
    "                # only update the non-termination state values (Ref: https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/)\n",
    "                next_state_values[non_final_mask] = self.target_net.net(non_final_next_states).max(1)[0] # extract max value\n",
    "                \n",
    "            # Update the model\n",
    "            expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "            self.policy_net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.policy_net.optimizer.step()\n",
    "\n",
    "            # Update the target Q-network in each 50 steps\n",
    "            self.c += 1\n",
    "            print(self.c)\n",
    "            if self.c % 50 == 0:\n",
    "                # print(self.test())\n",
    "                self.target_net.net.load_state_dict(self.policy_net.net.state_dict())\n",
    "\n",
    "    def test(self, n = 30,model_file=None):\n",
    "        # Evaluates the performance of the agent over 20 episodes.\n",
    "        rewards = []\n",
    "        for i in range(n):\n",
    "            max_t = 1000\n",
    "            state, _ = self.env.reset()\n",
    "            print(i)\n",
    "            for t in range(max_t):\n",
    "                state = torch.from_numpy(state).double().unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.policy_net.net(state)\n",
    "                action = self.greedy_policy(q_values)\n",
    "                state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                if terminated or truncated:\n",
    "                    rewards.append(reward)\n",
    "                    break\n",
    "\n",
    "        return np.average(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-19.346715923037028"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = DQN_Agent(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "training.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
