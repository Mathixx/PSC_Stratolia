# -*- coding: utf-8 -*-
"""Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1By4YZa_kpzTpY6zWajt1pPgIk13pe8Uq
"""

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import math
from torch.utils.data import Dataset
import pickle
import datetime
import pandas as pd

class ResBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dropout):
        super(ResBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

        self.bn1.weight.data.fill_(1)
        self.bn1.bias.data.zero_()
        self.bn2.weight.data.fill_(1)
        self.bn2.bias.data.zero_()

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out += identity
        out = self.relu(out)
        out = self.dropout(out)
        return out
        
        
        
class ResBlockBN(nn.Module):
    def __init__(self, channels, kernel_size, stride, padding, dropout):
        super(ResBlockBN, self).__init__()
        reduced_channel = int(channels/4)
        self.conv1 = nn.Conv2d(channels, reduced_channel, 1, 1, 0)
        self.bn1 = nn.BatchNorm2d(reduced_channel)
        self.conv2 = nn.Conv2d( reduced_channel,  reduced_channel, kernel_size, stride, padding)
        self.bn2 = nn.BatchNorm2d(reduced_channel)
        self.conv3 = nn.Conv2d( reduced_channel,  channels, 1, 1, 0)
        self.bn3 = nn.BatchNorm2d(channels)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

        self.bn1.weight.data.fill_(1)
        self.bn1.bias.data.zero_()
        self.bn2.weight.data.fill_(1)
        self.bn2.bias.data.zero_()
        self.bn3.weight.data.fill_(1)
        self.bn3.bias.data.zero_()

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        out += identity
        out = self.relu(out)
        out = self.dropout(out)
        return out
        
class ProjectedResBlockBN(nn.Module):
    def __init__(self, channels, kernel_size, stride, padding, dropout):
        super(ProjectedResBlockBN, self).__init__()
        reduced_channels = int(channels/2)
        self.conv1 = nn.Conv2d(channels, reduced_channels, 1, 1, 0)
        self.bn1 = nn.BatchNorm2d(reduced_channels)
        self.conv2 = nn.Conv2d( reduced_channels,  reduced_channels, kernel_size, 2 , padding)
        self.bn2 = nn.BatchNorm2d(reduced_channels)
        self.conv3 = nn.Conv2d( reduced_channels,  channels*2, 1, 1, 0)
        self.bn3 = nn.BatchNorm2d(channels*2)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        
        self.projector = nn.Conv2d( channels,  channels*2, kernel_size, 2 , padding)
        self.bnProj = nn.BatchNorm2d(channels*2)
        

        self.bn1.weight.data.fill_(1)
        self.bn1.bias.data.zero_()
        self.bn2.weight.data.fill_(1)
        self.bn2.bias.data.zero_()
        self.bn3.weight.data.fill_(1)
        self.bn3.bias.data.zero_()

    def forward(self, x):
        identity = x
        identity = self.projector(identity)
        identity = self.bnProj(identity)
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        
        out += identity
        out = self.relu(out)
        out = self.dropout(out)
        return out



class ProjectedResBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dropout):
        super(ProjectedResBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, 1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, 2, padding)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        self.projector = nn.Conv2d(in_channels,out_channels,kernel_size, stride = 2 , padding = padding)
        self.bn1.weight.data.fill_(1)
        self.bn1.bias.data.zero_()
        self.bn2.weight.data.fill_(1)
        self.bn2.bias.data.zero_()

    def forward(self, x):
        identity = x
        identity = self.projector(x)
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x += identity
        x = self.relu(x)
        x = self.dropout(x)
        return x

class Net12RD(nn.Module):
    def __init__(self , dir_number = 8, print_shape=False):
        self.print_shape = print_shape
        dropoutr = 0.10
        super(Net12RD, self).__init__()
        self.layers1 = nn.ModuleList( [
        nn.Conv3d(2, 64, (17,1,1)),
        nn.BatchNorm3d(64),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Conv3d(64, 64, (1,3,3), padding=(0,1,1)),
        nn.BatchNorm3d(64),
        nn.ReLU(),
        nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2))])
        self.layersRES = nn.ModuleList( [ResBlock(64,64, (3,3), 1, 1, dropoutr),
        ResBlock(64,64, (3,3), 1, 1, dropoutr),
        ProjectedResBlock(64,128, (3,3), 1, 1, dropoutr),
        ResBlock(128,128, (3,3), 1, 1, dropoutr),
        ResBlock(128,128, (3,3), 1, 1, dropoutr),
        ProjectedResBlock(128,256, (3,3), 1, 1, dropoutr),
        ResBlock(256,256, (3,3), 1, 1, dropoutr),
        ResBlock(256,256, (3,3), 1, 1, dropoutr),
        ProjectedResBlock(256,512, (3,3), 1, 0, dropoutr),
        ResBlock(512,512, (3,3), 1, 1, dropoutr),
        ResBlock(512,512, (3,3), 1, 1, dropoutr),
        nn.Conv2d(512, 512, kernel_size = (2,2), stride = (2,2)),
        nn.BatchNorm2d(512),
        nn.Dropout(dropoutr)])

        self.layers = nn.ModuleList([
        nn.Linear(4096, 2048),
        nn.BatchNorm1d(2048),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(2048, 1024),
        nn.BatchNorm1d(1024),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(1024, 1024),
        nn.BatchNorm1d(1024),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(1024, 512),
        nn.BatchNorm1d(512),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(512, dir_number)])

        for l in self.layers1:
            if isinstance(l, (nn.BatchNorm2d, nn.BatchNorm1d)):
                l.weight.data.fill_(1)
                l.bias.data.zero_()


        for l in self.layers:
            if isinstance(l, (nn.BatchNorm2d, nn.BatchNorm1d)):
                l.weight.data.fill_(1)
                l.bias.data.zero_()
            elif isinstance(l, nn.Conv2d):
                n = l.kernel_size[0] * l.kernel_size[1] * l.out_channels
                l.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(l, nn.Linear):
                nn.init.xavier_normal(l.weight)

    def forward(self, x):
        x = x[:,0,:,:,:,:]
        for l in self.layers1:
            x = l(x)
            if  (isinstance(l , ProjectedResBlock) or  isinstance(l , ResBlock) or  isinstance(l , nn.Dropout) ) and self.print_shape:
                print(x.shape)
        x = x[:,:,0,:,:]
        for l in self.layersRES :
            x = l(x)
            if  (isinstance(l , ProjectedResBlock) or  isinstance(l , ResBlock) or  isinstance(l , nn.Dropout) ) and self.print_shape:
                print(x.shape)
        x = torch.flatten(x, 1)
        for l in self.layers:
            x = l(x)
            if isinstance(l , nn.Dropout) and self.print_shape:
                print(x.shape)
        return x
        
        


class NetBT(nn.Module):
    def __init__(self , dir_number = 8, print_shape=False):
        self.print_shape = print_shape
        dropoutr = 0.1
        super(NetBT, self).__init__()
        self.layers1 = nn.ModuleList( [
        nn.Conv3d(2, 64, (17,1,1)),
        nn.BatchNorm3d(64),
        nn.ReLU(),
        nn.Dropout(dropoutr)])
        self.layersRES = nn.ModuleList( [
        ProjectedResBlock(64,128, (3,3), 1, 1, dropoutr),
        ResBlock(128,128, (3,3), 1, 1, dropoutr),
        ProjectedResBlock(128,256, (3,3), 1, 1, dropoutr),

        ResBlockBN(256, (3,3), 1, 1, dropoutr),
        ResBlockBN(256, (3,3), 1, 1, dropoutr),
        ProjectedResBlockBN(256, (3,3), 1,0, dropoutr),
        ResBlockBN(512, (3,3), 1, 1, dropoutr),
        ResBlockBN(512, (3,3), 1, 1, dropoutr),
        ProjectedResBlockBN(512, (3,3), 1, 0, dropoutr),
        ResBlockBN(1024, (3,3), 1, 1, dropoutr),
        ResBlockBN(1024, (3,3), 1, 1, dropoutr),
        ProjectedResBlockBN(1024, (3,3), 1, 1, dropoutr),
        ResBlockBN(2048, (3,3), 1, 1, dropoutr),
        ResBlockBN(2048, (3,3), 1, 1, dropoutr),
        ProjectedResBlockBN(2048, (3,3), 1, 1, dropoutr),
        nn.AdaptiveAvgPool2d(1)])

        self.layers = nn.ModuleList([
        nn.Linear(4096, 2048),
        nn.BatchNorm1d(2048),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(2048, 1024),
        nn.BatchNorm1d(1024),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(1024, 1024),
        nn.BatchNorm1d(1024),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(1024, 512),
        nn.BatchNorm1d(512),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(512, dir_number)])

        for l in self.layers1:
            if isinstance(l, (nn.BatchNorm2d, nn.BatchNorm1d)):
                l.weight.data.fill_(1)
                l.bias.data.zero_()


        for l in self.layers:
            if isinstance(l, (nn.BatchNorm2d, nn.BatchNorm1d)):
                l.weight.data.fill_(1)
                l.bias.data.zero_()
            elif isinstance(l, nn.Conv2d):
                n = l.kernel_size[0] * l.kernel_size[1] * l.out_channels
                l.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(l, nn.Linear):
                nn.init.xavier_normal(l.weight)

    def forward(self, x):
        x = x[:,0,:,:,:,:]
        for l in self.layers1:
            x = l(x)
            if  ( isinstance(l , ProjectedResBlock) or  isinstance(l , ResBlock) or  isinstance(l , ProjectedResBlockBN) or  isinstance(l , ResBlockBN) or isinstance(l , nn.Dropout) ) and self.print_shape:
                print(x.shape)
        x = x[:,:,0,:,:]
        for l in self.layersRES :
            x = l(x)
            if  (isinstance(l , ProjectedResBlock) or  isinstance(l , ResBlock) or isinstance(l , ProjectedResBlockBN) or  isinstance(l , ResBlockBN) or  isinstance(l , nn.Dropout) ) and self.print_shape:
                print(x.shape)
        x = torch.flatten(x, 1)
        for l in self.layers:
            x = l(x)
            if isinstance(l , nn.Dropout) and self.print_shape:
                print(x.shape)
        return x


class Net12RDDeep(nn.Module):
    def __init__(self , dir_number = 8, print_shape=False):
        self.print_shape = print_shape
        dropoutr = 0.10
        super(Net12RDDeep, self).__init__()
        self.layers1 = nn.ModuleList( [
        nn.Conv3d(2, 64, (17,1,1)),
        nn.BatchNorm3d(64),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Conv3d(64, 64, (1,3,3), padding=(0,1,1)),
        nn.BatchNorm3d(64),
        nn.ReLU(),
        nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2))])
        self.layersRES = nn.ModuleList( [ResBlock(64,64, (3,3), 1, 1, dropoutr),
        ResBlock(64,64, (3,3), 1, 1, dropoutr),
        ResBlock(64,64, (3,3), 1, 1, dropoutr),
        ProjectedResBlock(64,128, (3,3), 1, 1, dropoutr),
        ResBlock(128,128, (3,3), 1, 1, dropoutr),
        ResBlock(128,128, (3,3), 1, 1, dropoutr),
        ResBlock(128,128, (3,3), 1, 1, dropoutr),
        ProjectedResBlock(128,256, (3,3), 1, 1, dropoutr),
        ResBlock(256,256, (3,3), 1, 1, dropoutr),
        ResBlock(256,256, (3,3), 1, 1, dropoutr),
        ResBlock(256,256, (3,3), 1, 1, dropoutr),
        ResBlock(256,256, (3,3), 1, 1, dropoutr),
        ProjectedResBlock(256,512, (3,3), 1, 0, dropoutr),
        ResBlock(512,512, (3,3), 1, 1, dropoutr),
        ResBlock(512,512, (3,3), 1, 1, dropoutr),
        ResBlock(512,512, (3,3), 1, 1, dropoutr),
        nn.Conv2d(512, 512, kernel_size = (2,2), stride = (2,2)),
        nn.BatchNorm2d(512),
        nn.Dropout(dropoutr)])

        self.layers = nn.ModuleList([
        nn.Linear(4096, 2048),
        nn.BatchNorm1d(2048),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(2048, 1024),
        nn.BatchNorm1d(1024),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(1024, 1024),
        nn.BatchNorm1d(1024),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(1024, 512),
        nn.BatchNorm1d(512),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(512, dir_number)])

        for l in self.layers1:
            if isinstance(l, (nn.BatchNorm2d, nn.BatchNorm1d)):
                l.weight.data.fill_(1)
                l.bias.data.zero_()


        for l in self.layers:
            if isinstance(l, (nn.BatchNorm2d, nn.BatchNorm1d)):
                l.weight.data.fill_(1)
                l.bias.data.zero_()
            elif isinstance(l, nn.Conv2d):
                n = l.kernel_size[0] * l.kernel_size[1] * l.out_channels
                l.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(l, nn.Linear):
                nn.init.xavier_normal(l.weight)

    def forward(self, x):
        x = x[:,0,:,:,:,:]
        for l in self.layers1:
            x = l(x)
            if  (isinstance(l , ProjectedResBlock) or  isinstance(l , ResBlock) or  isinstance(l , nn.Dropout) ) and self.print_shape:
                print(x.shape)
        x = x[:,:,0,:,:]
        for l in self.layersRES :
            x = l(x)
            if  (isinstance(l , ProjectedResBlock) or  isinstance(l , ResBlock) or  isinstance(l , nn.Dropout) ) and self.print_shape:
                print(x.shape)
        x = torch.flatten(x, 1)
        for l in self.layers:
            x = l(x)
            if isinstance(l , nn.Dropout) and self.print_shape:
                print(x.shape)
        return x
        
        

class Net12RDShallow(nn.Module):
    def __init__(self , dir_number = 8, print_shape=False):
        self.print_shape = print_shape
        dropoutr = 0.1
        super(Net12RDShallow, self).__init__()
        self.layers1 = nn.ModuleList( [
        nn.Conv3d(2, 64, (17,1,1)),
        nn.BatchNorm3d(64),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Conv3d(64, 64, (1,3,3), padding=(0,1,1)),
        nn.BatchNorm3d(64),
        nn.ReLU(),
        nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2))])
        self.layersRES = nn.ModuleList( [ResBlock(64,64, (3,3), 1, 1, dropoutr),
        #ResBlock(64,64, (3,3), 1, 1, dropoutr),
        ProjectedResBlock(64,128, (3,3), 1, 1, dropoutr),
        ResBlock(128,128, (3,3), 1, 1, dropoutr),
        #ResBlock(128,128, (3,3), 1, 1, dropoutr),
        ProjectedResBlock(128,256, (3,3), 1, 1, dropoutr),
        #(256,256, (3,3), 1, 1, dropoutr),
        ResBlock(256,256, (3,3), 1, 1, dropoutr),
        ProjectedResBlock(256,512, (3,3), 1, 0, dropoutr),
        ResBlock(512,512, (3,3), 1, 1, dropoutr),
        #ResBlock(512,512, (3,3), 1, 1, dropoutr),
        nn.Conv2d(512, 512, kernel_size = (2,2), stride = (2,2)),
        nn.BatchNorm2d(512),
        nn.Dropout(dropoutr)])

        self.layers = nn.ModuleList([
        nn.Linear(4096, 2048),
        nn.BatchNorm1d(2048),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(2048, 1024),
        nn.BatchNorm1d(1024),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(1024, 1024),
        nn.BatchNorm1d(1024),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(1024, 512),
        nn.BatchNorm1d(512),
        nn.ReLU(),
        nn.Dropout(dropoutr),
        nn.Linear(512, dir_number)])

        for l in self.layers1:
            if isinstance(l, (nn.BatchNorm2d, nn.BatchNorm1d)):
                l.weight.data.fill_(1)
                l.bias.data.zero_()


        for l in self.layers:
            if isinstance(l, (nn.BatchNorm2d, nn.BatchNorm1d)):
                l.weight.data.fill_(1)
                l.bias.data.zero_()
            elif isinstance(l, nn.Conv2d):
                n = l.kernel_size[0] * l.kernel_size[1] * l.out_channels
                l.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(l, nn.Linear):
                nn.init.xavier_normal(l.weight)

    def forward(self, x):
        x = x[:,0,:,:,:,:]
        for l in self.layers1:
            x = l(x)
            if  (isinstance(l , ProjectedResBlock) or  isinstance(l , ResBlock) or  isinstance(l , nn.Dropout) ) and self.print_shape:
                print(x.shape)
        x = x[:,:,0,:,:]
        for l in self.layersRES :
            x = l(x)
            if  (isinstance(l , ProjectedResBlock) or  isinstance(l , ResBlock) or  isinstance(l , nn.Dropout) ) and self.print_shape:
                print(x.shape)
        x = torch.flatten(x, 1)
        for l in self.layers:
            x = l(x)
            if isinstance(l , nn.Dropout) and self.print_shape:
                print(x.shape)
        return x
        
    
class Net21D(nn.Module):
    def __init__(self , dir_number = 8, print_shape=False):
        self.print_shape = print_shape
        super(Net21D, self).__init__()
        self.layersCNN = nn.ModuleList( [
        nn.Conv3d(2, 64, (17,1,1)),
        nn.BatchNorm3d(64),
        nn.ReLU(),
        nn.Dropout(0.1),

        nn.Conv3d(64, 64, (1,3,3), padding=(0,1,1)),
        nn.BatchNorm3d(64),
        nn.ReLU(),
        nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2)),
        nn.Dropout(0.2),

        nn.Conv3d(64, 128, (1,3,3), padding=(0,1,1)),
        nn.BatchNorm3d(128),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Conv3d(128, 128, (1,3,3), padding=(0,0,0)),
        nn.BatchNorm3d(128),
        nn.ReLU(),
        nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2),padding=(0,0,1)),

        nn.Dropout(0.2),
        nn.Conv3d(128, 256, (1,3,3), padding=(0,1,1)),
        nn.BatchNorm3d(256),
        nn.ReLU(),
        nn.Dropout(0.2),

        nn.Conv3d(256, 256, (1,3,3), padding=(0,0,0)),
        nn.BatchNorm3d(256),
        nn.ReLU(),
        nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2),padding=(0,0,1)),
        nn.Dropout(0.2),

        nn.Conv3d(256, 512, (1,3,3), padding=(0,0,1)),
        nn.BatchNorm3d(512),
        nn.ReLU(),
        nn.MaxPool3d(kernel_size=(1,3,3), stride=(1,2,1)),
        nn.Dropout(0.2),
        nn.Conv3d(512, 512, (1,3,3), padding=(0,0,0)),
        nn.BatchNorm3d(512),
        nn.ReLU(),
        nn.MaxPool3d(kernel_size=(1,3,3), stride=(1,2,2)),
        nn.Dropout(0.2)

        ])
        self.layers = nn.ModuleList([
        nn.Linear(2048, 1024),
        nn.BatchNorm1d(1024),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(1024, 1024),
        nn.BatchNorm1d(1024),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(1024, 512),
        nn.BatchNorm1d(512),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(512, 512),
        nn.BatchNorm1d(512),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(512, dir_number)])

        for l in self.layers:
            if isinstance(l, (nn.BatchNorm3d, nn.BatchNorm1d)):
                l.weight.data.fill_(1)
                l.bias.data.zero_()
            elif isinstance(l, nn.Conv3d):
                n = l.kernel_size[0] * l.kernel_size[1] * l.kernel_size[2] * l.out_channels
                l.weight.data.normal_(0, math.sqrt(2. / n))
            elif isinstance(l, nn.Linear):
                nn.init.xavier_normal(l.weight)

    def forward(self, x):
        x = x[:,0,:,:,:,:]
        for l in self.layersCNN:
            x = l(x)
            if isinstance(l , nn.Dropout) and self.print_shape:
                print(x.shape)
        x = torch.flatten(x, 1)
        for l in self.layers:
            x = l(x)
            if isinstance(l , nn.Dropout) and self.print_shape:
                print(x.shape)
        return x