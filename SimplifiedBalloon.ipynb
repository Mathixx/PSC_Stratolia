{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules en lien avec Gym\n",
    "import gymnasium as gym\n",
    "\n",
    "#Modules utiles\n",
    "import sys\n",
    "sys.path.append('/Users/mathiasperez/Documents/GitHub/PSC_Stratolia/Algo Naif')\n",
    "from parcours import parcours_a_Z, distance_destination, case, ventU_ventV\n",
    "import Node\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "#Modules en lien avec Stable_baseline3\n",
    "#IMPORT THE TYPE OF AGENT U WANT TO USE\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.bis Import the wind-datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"objet_wind_data_2020.pickle\", \"rb\") as f:\n",
    "    wind_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create the environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Balloon(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(Balloon, self).__init__()\n",
    "\n",
    "        self.goal_long = 2.65\n",
    "        self.goal_lat = 34.54\n",
    "\n",
    "        self.limite_eloignement = 10000 #en m\n",
    "        self.precision = 1000 #en m\n",
    "\n",
    "        self.initial_time = datetime(2021, 1, 1, 0, 0, 0)\n",
    "\n",
    "        self.temps_exploration = 3600\n",
    "        \n",
    "        # Define the action space\n",
    "        self.action_space = gym.spaces.Discrete(3)  \n",
    "        \"\"\"\n",
    "        Action 0 : You do nothing and let the balloon follow the wind for temps_exploration seconds\n",
    "        Action 1 : You push the balloon upwards (goes up in altitude) (instanneous action)\n",
    "        Action 2 : You push the balloon downwards (goes down in altitude) (instanneous action)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define the observation space\n",
    "        self.observation_space = gym.spaces.Tuple(( \n",
    "            gym.spaces.Box(low=datetime(2021, 1, 1, 0, 0, 0), high=datetime(2022, 1, 1, 0, 0, 0), shape=(1,), dtype=datetime), # time (in special format)\n",
    "            gym.spaces.Discrete(18), #Altitude (17 different positions : no need for more precisions)\n",
    "            gym.spaces.Box(low=0, high=180, shape=(1,), dtype=np.float32),  #Longitude\n",
    "            gym.spaces.Box(low=0, high=90, shape=(1,), dtype=np.float32)   #Latitude\n",
    "            ))\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the environment to its initial state\n",
    "        # Resetting the ballon to its initial position : 73 boulevard des maréchaux\n",
    "        self.state = (np.array([self.initial_time], dtype=datetime), \n",
    "                      0, \n",
    "                      np.array([2.23], dtype=np.float32),\n",
    "                      np.array([50.23], dtype=np.float32)\n",
    "                      )\n",
    "        return self.state, {}\n",
    "    \n",
    "    def get_terminated(self, state):\n",
    "        long, lat = state[2], state[3]\n",
    "        if distance_destination((self.goal_long, self.goal_lat), long, lat) < self.limite_eloignement :\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        \"\"\"\n",
    "        - fonction qui donne récompense en fonction de la distance  a l'objectif par rapport a la distance initale\n",
    "        - ajoute un boolean si l'objectif a été atteint ou bien si on en est suffisamment proche\n",
    "        - d'autres trucs a faire surement\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Take a step in the environment based on the given action\n",
    "        if action == 0:\n",
    "            #You decide to follow the wind flow :\n",
    "            long, lat, pression = self.state[2], self.state[3], self.state[1]\n",
    "            secondes_depuis_debut = (self.state[0]-self.initial_time).total_seconds()\n",
    "            temps = secondes_depuis_debut//(6*3600), secondes_depuis_debut%(6*3600)\n",
    "            n = Node(long, lat,temps, pression, None) #longtiude, latitude, temps, ression, prev\n",
    "            terminated, n = parcours_a_Z(\n",
    "                                        (self.goal_long, self.goal_lat),\n",
    "                                        n,\n",
    "                                        temps_chgmt_pression = self.temps_exploration,\n",
    "                                        precision = self.precision, \n",
    "                                        tab_vent = wind_data)\n",
    "            done = terminated\n",
    "            new_date = self.initial_time + timedelta(seconds=n.temps[0]*(6*3600)+n.temps[1]) # A corriger\n",
    "            self.state = new_date, n.pression, n.long, n.lat\n",
    "\n",
    "        elif action == 1:\n",
    "            #You go up 1 case in altitude so u go down in pressure\n",
    "            self.state[1] -= 1\n",
    "        else :\n",
    "            #You go down one case in altitudeso u go up in pressure\n",
    "            self.state[1] += 1\n",
    "    \n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = get_reward(self.state)\n",
    "        \n",
    "        # Check if the episode has ended\n",
    "        if not done:\n",
    "            done = get_terminated(self.state)\n",
    "        \n",
    "        # Check if the episode has been truncated\n",
    "        truncated = False\n",
    "        \n",
    "        # Return the observation, reward, done flag, truncated flag, and info\n",
    "        return self.state, reward, done, truncated, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        # Render the environment\n",
    "        print(\"State:\", self.state)\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
