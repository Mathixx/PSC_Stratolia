{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Modules en lien avec Stable_baseline3\\n#IMPORT THE TYPE OF AGENT U WANT TO USE\\nfrom stable_baselines3.common.vec_env import DummyVecEnv\\nfrom stable_baselines3.common.evaluation import evaluate_policy\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Modules en lien avec Gym\n",
    "import gymnasium as gym\n",
    "\n",
    "#Modules utiles\n",
    "import sys\n",
    "sys.path.append('/Users/mathiasperez/Documents/GitHub/PSC_Stratolia/Algo Naif')\n",
    "from data_vent import ventU_ventV, ventU_ventV_interpolate, distance_destination\n",
    "import Node\n",
    "import Balloon\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import datetime as dt \n",
    "import time\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "#Modules en lien avec Stable_baseline3\n",
    "#IMPORT THE TYPE OF AGENT U WANT TO USE\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.bis Import the wind-datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLe tableau va de :\\ndatetime.datetime(2020, 1, 1, 0, 0) à datetime.datetime(2021, 1, 1, 0, 0)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"objet_wind_data_2020.pickle\", \"rb\") as f:\n",
    "    wind_data = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "Le tableau va de :\n",
    "datetime.datetime(2020, 1, 1, 0, 0) à datetime.datetime(2021, 1, 1, 0, 0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  10.   20.   30.   50.   70.  100.  150.  200.  250.  300.  400.  500.\n",
      "  600.  700.  850.  925. 1000.]\n"
     ]
    }
   ],
   "source": [
    "print((wind_data['metadata']['grid']['pressure']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.bisbis Create needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    INPUT : start_date and end_date -> datetime objects representing the start and end dates\n",
    "    OUTPUT : A random date bewtween the two inputs and an hour  that is a multiple of 6 (i.e., 0, 6, 12, or 18)\n",
    "\"\"\"\n",
    "def generate_random_date(start_date, end_date):\n",
    "    time_between_dates = end_date - start_date\n",
    "    days_between_dates = time_between_dates.days\n",
    "    random_number_of_days = random.randrange(days_between_dates)\n",
    "    random_date = start_date + dt.timedelta(days=random_number_of_days)\n",
    "    hr  = random.randrange(0, 24, 6)\n",
    "    random_time = dt.time(hour=hr, minute=0, second=0, microsecond=0, tzinfo=None, fold=0)\n",
    "    return dt.datetime.combine(random_date, random_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create the environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalloonEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    ## Description\n",
    "    This environment is a complete representation of our problem.\n",
    "\n",
    "    The destination is always at variable coordinates (random opr not).\n",
    "    So are the starting coordinatesv (probably)\n",
    "\n",
    "\n",
    "    ## Action Space\n",
    "    There are three discrete actions available:\n",
    "    - 0: do nothing et let the wind lead your movement\n",
    "    - 1: go up in altitude (17 heights allowed)\n",
    "    - 2: go down in altitude (17 heights allowed)\n",
    "\n",
    "    ## Observation Space\n",
    "    The state a tuple consisting of different parts : \n",
    "        First you have the time accessible using the datetime module\n",
    "        Then 2 Boxes (simple array) for the longitude and the latitude\n",
    "        Finally one discrete part consisting of the 71 different heights\n",
    "\n",
    "    ## Rewards\n",
    "    After every step a reward is granted. The total reward of an episode is the\n",
    "    sum of the rewards for all the steps within that episode.\n",
    "\n",
    "    For each step, the reward:\n",
    "    (- is increased/decreased the closer/further the ballon is to the goal point.)\n",
    "    - is increased/decreased the slower/faster the lander is moving.\n",
    "    (- is increased/decreased the less/more energy from the battery is used in the round)\n",
    "\n",
    "    The episode receive an additional reward of +100 or -100 points for finding the goal or not respectively.\n",
    "\n",
    "\n",
    "\n",
    "    ## Starting State\n",
    "    The lander starts at a random poinf of the globe (time can be random as well)\n",
    "\n",
    "    ## Episode Termination\n",
    "    The episode finishes if:\n",
    "    1) the balloon gets too far away from the goal;\n",
    "    2) the balloon goes near enough of the goal\n",
    "\n",
    "    ## Arguments\n",
    "\n",
    "     * `goal_long` and 'goal_lat' for the position of our goal destination (can be fixed for specific results)\n",
    "     * 'initial_time' defines the starting time of our episode (=> consequence on the winds)\n",
    "\n",
    "    * `precision' defines how close the balloon must be to the target before being considered as having reached it.\n",
    "\n",
    "    * 'limite_eloignement' defines how far the ballon must be from the target to consider the episode to be a failure.\n",
    "\n",
    "    * `temps_exploration` defines how long we let the ballon follow the winds when action : 0 is considered\n",
    "\n",
    "    ## Version History\n",
    "    - v0: Initial version\n",
    "\n",
    "    ## Current bugs\n",
    "\n",
    "    ## Credits\n",
    "    Mathias PEREZ et ses sbires\n",
    "    \"\"\"\n",
    "    def __init__(self, basStrat):\n",
    "        #super(Balloon, self).__init__()\n",
    "\n",
    "        self.basStrat : Balloon\n",
    "\n",
    "        self.initial_time : dt.datetime\n",
    "\n",
    "        self.limite_eloignement = 10000 #en m\n",
    "        self.precision = 1000 #en m\n",
    "\n",
    "        self.temps_exploration = 131 #secondes\n",
    "        \n",
    "        # Define the action space\n",
    "        self.action_space = gym.spaces.Discrete(3)  \n",
    "        \"\"\"\n",
    "        Action 0 : You do nothing and let the balloon follow the wind for temps_exploration seconds\n",
    "        Action 1 : You push the balloon upwards (goes up in altitude) (instanneous action)\n",
    "        Action 2 : You push the balloon downwards (goes down in altitude) (instanneous action)\n",
    "        \"\"\"\n",
    "        \n",
    "        taille_tab_vent : int\n",
    "\n",
    "        # Define the observation space\n",
    "        #firt we have position of the balloon, then the goal positon then the local wind data\n",
    "        low_obs = np.array([0, -90, 0, -90] + [-100, -50] * (taille_tab_vent))\n",
    "        high_obs = np.array([360,90,360,+90] + [100, 50] * (taille_tab_vent))\n",
    "        self.observation_space = gym.spaces.Tuple(( \n",
    "            gym.spaces.Box(low=low_obs, high=high_obs, dtype=np.float32), \n",
    "            gym.spaces.Discrete(17), #Altitude (17 different positions : no need for more precisions)\n",
    "            ))\n",
    "     \n",
    "    def reset(self, tab_vent : dict):\n",
    "        # Reset the environment to its initial state\n",
    "        # Resetting the ballon to a random initial position\n",
    "        #Ici on pourrait randomiser les goal et les positions de depart +temps\n",
    "        self.initial_time = generate_random_date(dt.datetime(2020, 1, 1, 0, 0), dt.datetime(2021, 1, 1, 0, 0))\n",
    "        \n",
    "        goal_long, goal_lat = random.uniform(0,360), random.uniform(-90,90)\n",
    "        depart_long, depart_lat = random.uniform(0,360), random.uniform(-90,90)\n",
    "        while distance_destination((self.goal_long, self.goal_lat), depart_long, depart_lat) > 10000000 :\n",
    "            goal_lat = random.uniform(-90,90)\n",
    "            goal_long = random.uniform(0,360)\n",
    "        \n",
    "        self.basStrat = Balloon(depart_long, depart_lat, self.initial_time, 16, None, precision = 1000)\n",
    "        \n",
    "        tab1 = [depart_long, depart_lat, goal_long, goal_lat]\n",
    "        tab2 = self.basStrat.get_winds(tab_vent)\n",
    "       \n",
    "        self.state = (\n",
    "                      np.concatenate(tab1, tab2),\n",
    "                      16\n",
    "                      )\n",
    "        return self.state, {}\n",
    "    \n",
    "    def get_terminated(self, state):\n",
    "        long, lat = state[0][0], state[0][1]\n",
    "        if distance_destination((self.goal_long, self.goal_lat), long, lat) < self.limite_eloignement :\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        \"\"\"\n",
    "        - fonction qui donne récompense en fonction de la distance  a l'objectif par rapport a la distance initale\n",
    "        - ajoute un boolean si l'objectif a été atteint ou bien si on en est suffisamment proche\n",
    "        - d'autres trucs a faire surement\n",
    "        \"\"\"\n",
    "        eloignement = distance_destination((self.state[0][2], self.state[0][3]), state[0][0], state[0][1])\n",
    "        time  =  self.basStrat.t.total_seconds() - self.initial_time.total_seconds()\n",
    "        if (eloignement <= self.limite_eloignement) :\n",
    "            reward =  1000 + math.log(1/time)\n",
    "        else :\n",
    "            reward = math.exp((self.limite_eloignement/eloignement)) +  math.log(1/time)\n",
    "        return reward\n",
    "\n",
    "    #### RENDER A RAJOUTER ICI\n",
    "    def step(self, action):\n",
    "        # Take a step in the environment based on the given action\n",
    "        if action == 0:\n",
    "            #You decide to follow the wind flow :\n",
    "            done = self.basStrat.follow_wind(self.temps_exploration, self.state[0][2], self.state[0][3], wind_data)\n",
    "            self.state[0][0], self.state[0][1] = self.basStrat.long, self.basStrat.lat\n",
    "\n",
    "        elif action == 1:\n",
    "            #You go up 1 case in altitude so u go down in pressure\n",
    "            done  = self.basStrat.up(self.temps_exploration,self.state[0][2], self.state[0][3], wind_data)\n",
    "            self.state[0][0], self.state[0][1] = self.basStrat.long, self.basStrat.lat #au cas ou on soit en limite haute\n",
    "        else :\n",
    "            #You go down one case in altitudeso u go up in pressure\n",
    "            done  = self.basStrat.down(self.temps_exploration,self.state[0][2], self.state[0][3], wind_data)\n",
    "            self.state[0][0], self.state[0][1] = self.basStrat.long, self.basStrat.lat #au cas ou on soit en limite basse\n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = get_reward(self.state)\n",
    "        \n",
    "        # Check if the episode has ended\n",
    "        if not done:\n",
    "            done = get_terminated(self.state)\n",
    "        \n",
    "        # Check if the episode has been truncated\n",
    "        truncated = False\n",
    "        \n",
    "        # Return the observation, reward, done flag, truncated flag, and info\n",
    "        return self.state, reward, done, truncated, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        # Render the environment\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(31644000), Discrete(18), Box(0.0, 360.0, (1,), float32), Box(-90.0, 90.0, (1,), float32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = BalloonEnv()\n",
    "\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
