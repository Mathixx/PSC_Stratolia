{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import torch \n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from typing import List, Tuple\n",
    "from matplotlib import animation\n",
    "\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "from gym.wrappers import FlattenObservation\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baloon1:\n",
    "    def __init__(self,width,length,height):\n",
    "        self.x, self.y , self.z = 0,0,0\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.length = length\n",
    "        self.map = None\n",
    "\n",
    "    def resest(self,start_x,start_y,start_z):\n",
    "        self.x, self.y ,self.z = start_x,start_y,start_z\n",
    "\n",
    "    def up(self):\n",
    "        self.z = min(self.height,self.z+1)\n",
    "\n",
    "    def down(self):\n",
    "        if(self.y>0):\n",
    "            self.z = max(self.z-1,0)\n",
    "\n",
    "    def step(self):\n",
    "        self.x = (self.x + self.map[self.x,self.y,self.z,0])% self.width\n",
    "        self.y = (self.y + self.map[self.x,self.y,self.z,1])% self.length\n",
    "\n",
    "    def generate_map(self,eta,mu):\n",
    "        self.map = np.zeros((self.width,self.length,self.height,2))\n",
    "        for i in range(self.width):\n",
    "            for j in  range(self.length):\n",
    "                for k in range(self.height):\n",
    "                    for l in range(1):\n",
    "                        r = np.random.random()\n",
    "                        if(r>eta):\n",
    "                            r= np.random.random()\n",
    "                            if(r>mu):\n",
    "                                self.map[i,j,k,l] = 1\n",
    "                            else:\n",
    "                                self.map[i,j,k,l] = -1\n",
    "\n",
    "\n",
    "    def set_map(self,map):\n",
    "        self.map = map\n",
    "\n",
    "    def get_winds(self,r):\n",
    "        obs = np.zeros((2*r+1,2*r+1,self.height,2))\n",
    "        for i in range(self.x - r , self.x + r + 1):\n",
    "            for j in range(self.y - r, self.y + r + 1):\n",
    "                for k in range(0,self.height):\n",
    "                    for l in range(1):\n",
    "                        obs[j - self.y + r, i-self.x + r]  = (self.map[i%self.width,j%self.length,k,l])\n",
    "        return obs\n",
    "    \n",
    "    def render(self):\n",
    "        for z in range(self.height):\n",
    "            for j in range(self.length):\n",
    "                s = ''\n",
    "                for i in range(self.width):\n",
    "                    s+= \"[\" + str(self.map[i,j,z]) + \"]\"\n",
    "                    \"\"\"if self.x == j and self.y == i : \n",
    "                        s+= '+'\n",
    "                    else:\n",
    "                        if abs(self.map[i,j,z,1]) == abs(self.map[i,j,z,0]) == 1:\n",
    "                            s += '/'\n",
    "                        elif self.map[i,j,z,1] == self.map[i,j,z,0] == 0:\n",
    "                            s += '.'\n",
    "                        elif abs(self.map[i,j,z,1]) == 1 and self.map[i,j,z,0] == 0 :\n",
    "                            s += '|'\n",
    "                        elif abs(self.map[i,j,z,1]) == 0 and self.map[i,j,z,0] == 1 :\n",
    "                            s += '-'\n",
    "                        elif self.map[i,j,z,1] * self.map[i,j,z,0] == -1 :\n",
    "                            s += '\\\\'\n",
    "                print(s)\n",
    "            s+= '\\n'\"\"\"\n",
    "                print(s)\n",
    "            print(\" \")\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]][[0. 0.]][[-1.  0.]][[1. 0.]][[-1.  0.]]\n",
      "[[-1.  0.]][[0. 0.]][[0. 0.]][[-1.  0.]][[1. 0.]]\n",
      "[[0. 0.]][[0. 0.]][[-1.  0.]][[0. 0.]][[0. 0.]]\n",
      " \n",
      "[[0. 0.]][[0. 0.]][[0. 0.]][[0. 0.]][[1. 0.]]\n",
      "[[-1.  0.]][[0. 0.]][[1. 0.]][[0. 0.]][[0. 0.]]\n",
      "[[-1.  0.]][[-1.  0.]][[-1.  0.]][[0. 0.]][[0. 0.]]\n",
      " \n",
      "[[-1.  0.]][[-1.  0.]][[1. 0.]][[1. 0.]][[0. 0.]]\n",
      "[[-1.  0.]][[-1.  0.]][[-1.  0.]][[1. 0.]][[-1.  0.]]\n",
      "[[1. 0.]][[0. 0.]][[1. 0.]][[0. 0.]][[-1.  0.]]\n",
      " \n",
      "[[0. 0.]][[-1.  0.]][[-1.  0.]][[0. 0.]][[0. 0.]]\n",
      "[[0. 0.]][[0. 0.]][[0. 0.]][[0. 0.]][[1. 0.]]\n",
      "[[1. 0.]][[1. 0.]][[0. 0.]][[-1.  0.]][[-1.  0.]]\n",
      " \n",
      "[[1. 0.]][[1. 0.]][[0. 0.]][[0. 0.]][[1. 0.]]\n",
      "[[1. 0.]][[0. 0.]][[0. 0.]][[0. 0.]][[0. 0.]]\n",
      "[[0. 0.]][[0. 0.]][[1. 0.]][[-1.  0.]][[0. 0.]]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "bal = baloon1(5,5,3)\n",
    "bal.generate_map(0.5,0.5)\n",
    "bal.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    def __init__(self, render_mode=None, size=10, obs_size = 5 ):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.balloon = baloon1()\n",
    "        self.obs_size = obs_size\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"map\": spaces.Box(-1, 1, shape=(2*obs_size + 1,obs_size*2+1), dtype=int),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def reset(self):\n",
    "        self.balloon.generate_map(self.size,self.size , 0.5)\n",
    "\n",
    "        self.time = 0\n",
    "\n",
    "        self._agent_location = [2,2]\n",
    "\n",
    "        self._target_location = [self.size-1,self.size-1]\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.time += 1\n",
    "\n",
    "        if action == 0:\n",
    "            self.balloon.step()\n",
    "        if action == 1:\n",
    "            self.balloon.up()\n",
    "        if action == 2:\n",
    "            self.balloon.down()\n",
    "\n",
    "        self._agent_location = np.array([self.balloon.x , self.balloon.y])\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        reward = -(self.time + np.sqrt((self._agent_location[0] - self._target_location[0])**2 + (self._agent_location[1] - self._target_location[1])**2))\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation, reward, terminated, False, {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location , \"map\" : self.balloon.get_winds(self.obs_size)}\n",
    "\n",
    "    def render(self):\n",
    "        self.balloon.render()\n",
    "        \n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.soft = torch.nn.Softmax(dim = -1)\n",
    "        # Define layers with ReLU activation\n",
    "        self.linear1 = nn.Linear(input_size, 30)\n",
    "        self.activation1 = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(30, 30)\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "        self.linear3 = nn.Linear(30, 30)\n",
    "        self.activation3 = nn.Sigmoid()\n",
    "\n",
    "        # Output layer without activation function\n",
    "        self.output_layer = nn.Linear(30, output_size)\n",
    "\n",
    "        # Initialization using Xavier uniform (a popular technique for initializing weights in NNs)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        nn.init.xavier_normal_(self.linear3.weight)\n",
    "        nn.init.xavier_normal_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the layers\n",
    "        x = self.activation1(self.linear1(inputs))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        x = self.soft(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class QNetwork:\n",
    "    def __init__(self, env, lr, logdir=None):\n",
    "        # Define Q-network with specified architecture\n",
    "        self.net = FullyConnectedModel(4, 2)\n",
    "        self.env = env\n",
    "        self.lr = lr \n",
    "        self.logdir = logdir\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "\n",
    "    def load_model(self, model_file):\n",
    "        # Load pre-trained model from a file\n",
    "        return self.net.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    def load_model_weights(self, weight_file):\n",
    "        # Load pre-trained model weights from a file\n",
    "        return self.net.load_state_dict(torch.load(weight_file))\n",
    "    \n",
    "class QPolicy:\n",
    "    def __init__(self, s_size, a_size):\n",
    "        self.net = FullyConnectedModel(s_size,a_size)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.tensor(state)\n",
    "        probs = self.net(state)\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every,env):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_training_episodes + 1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state, _ = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(torch.tensor(state,dtype=torch.double))\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, truncated , _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done or truncated:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as\n",
    "        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "        #\n",
    "        # In O(N) time, where N is the number of time steps\n",
    "        # (this definition of the discounted return G_t follows the definition of this quantity\n",
    "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_t = r_(t+1) + r_(t+2) + ...\n",
    "\n",
    "        # Given this formulation, the returns at each timestep t can be computed\n",
    "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
    "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
    "        # G_(t-1) = r_t + gamma* G_t\n",
    "        # (this follows a dynamic programming approach, with which we memorize solutions in order\n",
    "        # to avoid computing them multiple times)\n",
    "\n",
    "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
    "\n",
    "        ## Given the above, we calculate the returns at timestep t as:\n",
    "        #               gamma[t] * return[t] + reward[t]\n",
    "        #\n",
    "        ## We compute this starting from the last timestep to the first, in order\n",
    "        ## to employ the formula presented above and avoid redundant computations that would be needed\n",
    "        ## if we were to do it from first to last.\n",
    "\n",
    "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
    "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
    "        ## a normal python list would instead require O(N) to do this.\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = returns[0] if len(returns) > 0 else 0\n",
    "            returns.appendleft(gamma * disc_return_t + rewards[t])\n",
    "\n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        ## eps is the smallest representable float, which is\n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        # Line 7:\n",
    "        policy_loss = 0\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss += (-log_prob * disc_return)\n",
    "\n",
    "        # Line 8: PyTorch prefers gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(policy.net.linear3.weight)\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "\n",
    "from gym.wrappers import FlattenObservation\n",
    "\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 10,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-7,\n",
    "    \"grid_size\" : 5,\n",
    "    \"obs_r\" :3,\n",
    "    \"action_space\": 3,\n",
    "    \"print\" : 1\n",
    "}\n",
    "\n",
    "policy = QPolicy(\n",
    "    (hyperparameters[\"obs_r\"]*2+1)**2 + 4,hyperparameters[\"action_space\"]\n",
    ")\n",
    "optimizer = optim.Adam(policy.net.parameters(), lr=hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monce\\AppData\\Local\\Temp\\ipykernel_28132\\1435207334.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: -94.22\n",
      "Episode 2\tAverage Score: -95.27\n",
      "Episode 3\tAverage Score: -91.75\n",
      "Episode 4\tAverage Score: -93.22\n",
      "Episode 5\tAverage Score: -93.84\n",
      "Episode 6\tAverage Score: -93.25\n",
      "Episode 7\tAverage Score: -94.11\n",
      "Episode 8\tAverage Score: -95.37\n",
      "Episode 9\tAverage Score: -95.62\n",
      "Episode 10\tAverage Score: -94.59\n",
      "Episode 11\tAverage Score: -94.17\n",
      "Episode 12\tAverage Score: -93.73\n",
      "Episode 13\tAverage Score: -94.64\n",
      "Episode 14\tAverage Score: -95.38\n",
      "Episode 15\tAverage Score: -94.82\n",
      "Episode 16\tAverage Score: -94.56\n",
      "Episode 17\tAverage Score: -94.10\n",
      "Episode 18\tAverage Score: -93.58\n",
      "Episode 19\tAverage Score: -93.41\n",
      "Episode 20\tAverage Score: -92.29\n",
      "Episode 21\tAverage Score: -91.70\n",
      "Episode 22\tAverage Score: -90.89\n",
      "Episode 23\tAverage Score: -90.75\n",
      "Episode 24\tAverage Score: -88.65\n",
      "Episode 25\tAverage Score: -85.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monce\\AppData\\Local\\Temp\\ipykernel_28132\\450940638.py:58: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  returns = (returns - returns.mean()) / (returns.std() + eps)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (3,)) of distribution Categorical(probs: torch.Size([3])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan, nan], grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[384], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m GridWorldEnv(size \u001b[38;5;241m=\u001b[39m hyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], obs_size\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_r\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      2\u001b[0m wrapped_env \u001b[38;5;241m=\u001b[39m FlattenObservation(env)\n\u001b[1;32m----> 4\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mreinforce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_training_episodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_t\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapped_env\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[382], line 9\u001b[0m, in \u001b[0;36mreinforce\u001b[1;34m(policy, optimizer, n_training_episodes, max_t, gamma, print_every, env)\u001b[0m\n\u001b[0;32m      7\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_t):\n\u001b[1;32m----> 9\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     saved_log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n\u001b[0;32m     11\u001b[0m     state, reward, done, truncated , _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn[376], line 55\u001b[0m, in \u001b[0;36mQPolicy.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     53\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state)\n\u001b[0;32m     54\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(state)\n\u001b[1;32m---> 55\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m action \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem(), m\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[1;32mc:\\Users\\monce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\distributions\\categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     69\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\monce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\distributions\\distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (3,)) of distribution Categorical(probs: torch.Size([3])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan, nan], grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "env = GridWorldEnv(size = hyperparameters[\"grid_size\"], obs_size=hyperparameters[\"obs_r\"])\n",
    "wrapped_env = FlattenObservation(env)\n",
    "\n",
    "scores = reinforce(\n",
    "    policy,\n",
    "    optimizer,\n",
    "    hyperparameters[\"n_training_episodes\"],\n",
    "    hyperparameters[\"max_t\"],\n",
    "    hyperparameters[\"gamma\"],\n",
    "    hyperparameters[\"print\"],\n",
    "    wrapped_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wrapped_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_env\u001b[49m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(policy\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mlinear1\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m      3\u001b[0m (torch\u001b[38;5;241m.\u001b[39mtensor(state,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wrapped_env' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "state, _ = wrapped_env.reset()\n",
    "print(policy.net.linear1.weight)\n",
    "(torch.tensor(state,dtype=torch.double))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+><><\n",
      "<<><<\n",
      "><<>>\n",
      "><<>>\n",
      "<<>><\n",
      "\n",
      ">+<><\n",
      "<<><<\n",
      "><<>>\n",
      "><<>>\n",
      "<<>><\n",
      "\n",
      " \n",
      "-6.0\n",
      " \n",
      ">+<><\n",
      "<<><<\n",
      "><<>>\n",
      "><<>>\n",
      "<<>><\n",
      "\n",
      " \n",
      "-7.0\n",
      " \n",
      ">+<><\n",
      "<<><<\n",
      "><<>>\n",
      "><<>>\n",
      "<<>><\n",
      "\n",
      " \n",
      "-8.0\n",
      " \n",
      ">+<><\n",
      "<<><<\n",
      "><<>>\n",
      "><<>>\n",
      "<<>><\n",
      "\n",
      " \n",
      "-9.0\n",
      " \n",
      ">><><\n",
      "<-><<\n",
      "><<>>\n",
      "><<>>\n",
      "<<>><\n",
      "\n",
      " \n",
      "-9.242640687119284\n",
      " \n"
     ]
    }
   ],
   "source": [
    "env2 = GridWorldEnv(size = hyperparameters[\"grid_size\"], obs_size=hyperparameters[\"obs_r\"])\n",
    "env2.reset()\n",
    "env2.render()\n",
    "for i in range(5):\n",
    "    state, reward, done, truncated , _ =env2.step(env2.action_space.sample())\n",
    "    env2.render()\n",
    "    print(\" \")\n",
    "    print(reward)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2.action_space.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
