{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from typing import List, Tuple\n",
    "\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "\n",
    "        # Define layers with ReLU activation\n",
    "        self.linear1 = nn.Linear(input_size, 16)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(16, 16)\n",
    "        self.activation3 = nn.ReLU()\n",
    "\n",
    "        # Output layer without activation function\n",
    "        self.output_layer = nn.Linear(16, output_size)\n",
    "\n",
    "        # Initialization using Xavier uniform (a popular technique for initializing weights in NNs)\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        nn.init.xavier_uniform_(self.linear3.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the layers\n",
    "        x = self.activation1(self.linear1(inputs))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class QNetwork:\n",
    "    def __init__(self, env, lr, logdir=None):\n",
    "        # Define Q-network with specified architecture\n",
    "        self.net = FullyConnectedModel(4, 2)\n",
    "        self.env = env\n",
    "        self.lr = lr \n",
    "        self.logdir = logdir\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "\n",
    "    def load_model(self, model_file):\n",
    "        # Load pre-trained model from a file\n",
    "        return self.net.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    def load_model_weights(self, weight_file):\n",
    "        # Load pre-trained model weights from a file\n",
    "        return self.net.load_state_dict(torch.load(weight_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, env, memory_size=50000, burn_in=10000):\n",
    "        # Initializes the replay memory, which stores transitions recorded from the agent taking actions in the environment.\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.memory = collections.deque([], maxlen=memory_size)\n",
    "        self.env = env\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        # Returns a batch of randomly sampled transitions to be used for training the model.\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def append(self, transition):\n",
    "        # Appends a transition to the replay memory.\n",
    "        self.memory.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class DQN_Agent:\n",
    "\n",
    "    def __init__(self, env, lr=5e-4, render=False):\n",
    "        # Initialize the DQN Agent.\n",
    "        self.env = env\n",
    "        self.lr = lr\n",
    "        self.policy_net = QNetwork(self.env, self.lr)\n",
    "        self.target_net = QNetwork(self.env, self.lr)\n",
    "        self.target_net.net.load_state_dict(self.policy_net.net.state_dict())  # Copy the weight of the policy network\n",
    "        self.rm = ReplayMemory(self.env)\n",
    "        self.burn_in_memory()\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.c = 0\n",
    "\n",
    "    def burn_in_memory(self):\n",
    "        # Initialize replay memory with a burn-in number of episodes/transitions.\n",
    "        cnt = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        state, _ = self.env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Iterate until we store \"burn_in\" buffer\n",
    "        while cnt < self.rm.burn_in:\n",
    "            # Reset environment if terminated or truncated\n",
    "            if terminated or truncated:\n",
    "                state, _ = self.env.reset()\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "            # Randomly select an action (left or right) and take a step\n",
    "            action = torch.tensor(self.env.action_sapce.sample()).reshape(1, 1)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "            reward = torch.tensor([reward])\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "                \n",
    "            # Store new experience into memory\n",
    "            transition = Transition(state, action, next_state, reward)\n",
    "            self.rm.memory.append(transition)\n",
    "            state = next_state\n",
    "            cnt += 1\n",
    "\n",
    "    def epsilon_greedy_policy(self, q_values, epsilon=0.05):\n",
    "        # Implement an epsilon-greedy policy. \n",
    "        p = random.random()\n",
    "        if p > epsilon:\n",
    "            with torch.no_grad():\n",
    "                return self.greedy_policy(q_values)\n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)\n",
    "\n",
    "    def greedy_policy(self, q_values):\n",
    "        # Implement a greedy policy for test time.\n",
    "        return torch.argmax(q_values)\n",
    "        \n",
    "    def train(self):\n",
    "        # Train the Q-network using Deep Q-learning.\n",
    "        state, _ = self.env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Loop until reaching the termination state\n",
    "        while not (terminated or truncated):\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net.net(state)\n",
    "\n",
    "            # Decide the next action with epsilon greedy strategy\n",
    "            action = self.epsilon_greedy_policy(q_values).reshape(1, 1)\n",
    "            \n",
    "            # Take action and observe reward and next state\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "            reward = torch.tensor([reward])\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            # Store the new experience\n",
    "            transition = Transition(state, action, next_state, reward)\n",
    "            self.rm.memory.append(transition)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Sample minibatch with size N from memory\n",
    "            transitions = self.rm.sample_batch(self.batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "            # Get current and next state values\n",
    "            state_action_values = self.policy_net.net(state_batch).gather(1, action_batch) # extract values corresponding to the actions Q(S_t, A_t)\n",
    "            next_state_values = torch.zeros(self.batch_size)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # no next_state_value update if an episode is terminated (next_satate = None)\n",
    "                # only update the non-termination state values (Ref: https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/)\n",
    "                next_state_values[non_final_mask] = self.target_net.net(non_final_next_states).max(1)[0] # extract max value\n",
    "                \n",
    "            # Update the model\n",
    "            expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "            self.policy_net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.policy_net.optimizer.step()\n",
    "\n",
    "            # Update the target Q-network in each 50 steps\n",
    "            self.c += 1\n",
    "            if self.c % 50 == 0:\n",
    "                self.target_net.net.load_state_dict(self.policy_net.net.state_dict())\n",
    "\n",
    "    def test(self, model_file=None):\n",
    "        # Evaluates the performance of the agent over 20 episodes.\n",
    "\n",
    "        max_t = 1000\n",
    "        state, _ = self.env.reset()\n",
    "        rewards = []\n",
    "\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net.net(state)\n",
    "            action = self.greedy_policy(q_values)\n",
    "            state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        return np.sum(rewards)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
