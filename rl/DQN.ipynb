{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from typing import List, Tuple\n",
    "from gym import spaces\n",
    "\n",
    "from gym.wrappers import FlattenObservation\n",
    "\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "\n",
    "        # Define layers with ReLU activation\n",
    "        self.linear1 = nn.Linear(input_size, 200)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(200, 200)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(200, 50)\n",
    "        self.activation3 = nn.ReLU()\n",
    "\n",
    "        # Output layer without activation function\n",
    "        self.output_layer = nn.Linear(50, output_size)\n",
    "\n",
    "        # Initialization using Xavier uniform (a popular technique for initializing weights in NNs)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        nn.init.xavier_normal_(self.linear3.weight)\n",
    "        nn.init.xavier_normal_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the layers\n",
    "        x = self.activation1(self.linear1(inputs))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class QNetwork:\n",
    "    def __init__(self, env,  lr, input=150 , logdir=None):\n",
    "        # Define Q-network with specified architecture\n",
    "        self.net = FullyConnectedModel(input, 3)\n",
    "        self.env = env\n",
    "        self.lr = lr \n",
    "        self.logdir = logdir\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "\n",
    "    def load_model(self, model_file):\n",
    "        # Load pre-trained model from a file\n",
    "        return self.net.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    def load_model_weights(self, weight_file):\n",
    "        # Load pre-trained model weights from a file\n",
    "        return self.net.load_state_dict(torch.load(weight_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, env, memory_size=50000, burn_in=10000):\n",
    "        # Initializes the replay memory, which stores transitions recorded from the agent taking actions in the environment.\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.memory = collections.deque([], maxlen=memory_size)\n",
    "        self.env = env\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        # Returns a batch of randomly sampled transitions to be used for training the model.\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def append(self, transition):\n",
    "        # Appends a transition to the replay memory.\n",
    "        self.memory.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class DQN_Agent:\n",
    "\n",
    "    def __init__(self, env, lr=5e-4, render=False):\n",
    "        # Initialize the DQN Agent.\n",
    "        self.env = env\n",
    "        self.lr = lr\n",
    "        self.policy_net = QNetwork(self.env, self.lr)\n",
    "        self.target_net = QNetwork(self.env, self.lr)\n",
    "        self.target_net.net.load_state_dict(self.policy_net.net.state_dict())  # Copy the weight of the policy network\n",
    "        self.rm = ReplayMemory(self.env)\n",
    "        self.burn_in_memory()\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.c = 0\n",
    "\n",
    "    def burn_in_memory(self):\n",
    "        # Initialize replay memory with a burn-in number of episodes/transitions.\n",
    "        cnt = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        state, _ = self.env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.double).unsqueeze(0)\n",
    "\n",
    "        # Iterate until we store \"burn_in\" buffer\n",
    "        while cnt < self.rm.burn_in:\n",
    "            # Reset environment if terminated or truncated\n",
    "            if terminated or truncated:\n",
    "                state, _ = self.env.reset()\n",
    "                state = torch.tensor(state, dtype=torch.double).unsqueeze(0)\n",
    "            \n",
    "            # Randomly select an action (left or right) and take a step\n",
    "            action = torch.tensor(self.env.action_space.sample()).reshape(1, 1)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "            reward = torch.tensor([reward])\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.double).unsqueeze(0)\n",
    "                \n",
    "            # Store new experience into memory\n",
    "            transition = Transition(state, action, next_state, reward)\n",
    "            self.rm.memory.append(transition)\n",
    "            state = next_state\n",
    "            cnt += 1\n",
    "\n",
    "    def epsilon_greedy_policy(self, q_values, epsilon=0.05):\n",
    "        # Implement an epsilon-greedy policy. \n",
    "        p = random.random()\n",
    "        if p > epsilon:\n",
    "            with torch.no_grad():\n",
    "                return self.greedy_policy(q_values)\n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)\n",
    "\n",
    "    def greedy_policy(self, q_values):\n",
    "        # Implement a greedy policy for test time.\n",
    "        return torch.argmax(q_values)\n",
    "        \n",
    "    def train(self):\n",
    "        # Train the Q-network using Deep Q-learning.\n",
    "        state, _ = self.env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.double).unsqueeze(0)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Loop until reaching the termination state\n",
    "        while not (terminated or truncated):\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net.net(state)\n",
    "\n",
    "            # Decide the next action with epsilon greedy strategy\n",
    "            action = self.epsilon_greedy_policy(q_values).reshape(1, 1)\n",
    "            \n",
    "            # Take action and observe reward and next state\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "            reward = torch.tensor([reward])\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.double).unsqueeze(0)\n",
    "\n",
    "            # Store the new experience\n",
    "            transition = Transition(state, action, next_state, reward)\n",
    "            self.rm.memory.append(transition)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Sample minibatch with size N from memory\n",
    "            transitions = self.rm.sample_batch(self.batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "            # Get current and next state values\n",
    "            state_action_values = self.policy_net.net(state_batch).gather(1, action_batch) # extract values corresponding to the actions Q(S_t, A_t)\n",
    "            next_state_values = torch.zeros(self.batch_size)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # no next_state_value update if an episode is terminated (next_satate = None)\n",
    "                # only update the non-termination state values (Ref: https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/)\n",
    "                next_state_values[non_final_mask] = self.target_net.net(non_final_next_states).max(1)[0] # extract max value\n",
    "                \n",
    "            # Update the model\n",
    "            expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "            self.policy_net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.policy_net.optimizer.step()\n",
    "\n",
    "            # Update the target Q-network in each 50 steps\n",
    "            self.c += 1\n",
    "            if self.c % 50 == 0:\n",
    "                print(self.test())\n",
    "                self.target_net.net.load_state_dict(self.policy_net.net.state_dict())\n",
    "\n",
    "    def test(self, n = 30,model_file=None):\n",
    "        # Evaluates the performance of the agent over 20 episodes.\n",
    "        rewards = []\n",
    "        for i in range(n):\n",
    "            max_t = 1000\n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            for t in range(max_t):\n",
    "                state = torch.from_numpy(state).double().unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.policy_net.net(state)\n",
    "                action = self.greedy_policy(q_values)\n",
    "                state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                if terminated or truncated:\n",
    "                    rewards.append(reward)\n",
    "                    break\n",
    "\n",
    "        return np.average(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class balloon3D:\n",
    "    def __init__(self,width,length,height,obs_size):\n",
    "        self.obs_size = obs_size\n",
    "        self.x, self.y , self.z = 0,0,0\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.length = length\n",
    "        self.map = None\n",
    "\n",
    "    def reset(self,start_x,start_y,start_z):\n",
    "        self.x, self.y ,self.z = start_x,start_y,start_z\n",
    "\n",
    "    def up(self):\n",
    "        self.z = min(self.height-1,self.z+1)\n",
    "\n",
    "    def down(self):\n",
    "        self.z = max(self.z-1,0)\n",
    "\n",
    "    def step(self):\n",
    "        newx = int((self.x + self.map[self.x,self.y,self.z,0]) % self.width)\n",
    "        newy = int((self.y + self.map[self.x,self.y,self.z,1]) % self.length)\n",
    "        self.x = newx\n",
    "        self.y = newy\n",
    "\n",
    "    def generate_map(self,eta,mu):\n",
    "        self.map = np.zeros((self.width,self.length,self.height,2))\n",
    "        for i in range(self.width):\n",
    "            for j in  range(self.length):\n",
    "                for k in range(self.height):\n",
    "                    for l in range(2):\n",
    "                        r = np.random.random()\n",
    "                        if(r>eta):\n",
    "                            r= np.random.random()\n",
    "                            if(r>mu):\n",
    "                                self.map[i,j,k,l] = 1\n",
    "                            else:\n",
    "                                self.map[i,j,k,l] = -1\n",
    "\n",
    "\n",
    "    def set_map(self,map):\n",
    "        self.map = map\n",
    "\n",
    "    def get_winds(self):\n",
    "        r = self.obs_size\n",
    "        self.obs = np.zeros((2*r+1,2*r+1,self.height,2))\n",
    "        for i in range(self.x - r , self.x + r + 1):\n",
    "            for j in range(self.y - r, self.y + r + 1):\n",
    "                for k in range(0,self.height):\n",
    "                    for l in range(2):\n",
    "                        self.obs[i - self.x + r, j- self.y + r,k,l]  = (self.map[i%self.width,j%self.length,k,l])\n",
    "        return self.obs\n",
    "    \n",
    "    def render_obs(self):\n",
    "        r =self.obs_size*2+1\n",
    "        for z in range(self.height):\n",
    "            for j in range(r):\n",
    "                s = ''\n",
    "                for i in range(r):\n",
    "                    l = ''\n",
    "                    if(self.obs[i,j,z,0] == 0):\n",
    "                        l += '.'\n",
    "                    elif(self.obs[i,j,z,0] == 1):\n",
    "                        l+='>'\n",
    "                    elif(self.obs[i,j,z,0] == -1):\n",
    "                        l+='<'\n",
    "\n",
    "                    if(self.obs[i,j,z,1] == 0):\n",
    "                        l += '.'\n",
    "                    elif(self.obs[i,j,z,1] == 1):\n",
    "                        l+='v'\n",
    "                    elif(self.obs[i,j,z,1] == -1):\n",
    "                        l+='^'                  \n",
    "                    s+= l+ \" \"\n",
    "                print(s)\n",
    "            print(\" \")\n",
    "    \n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        for z in range(self.height):\n",
    "            for j in range(self.length):\n",
    "                s = ''\n",
    "                for i in range(self.width):\n",
    "                    if self.x == i and self.y == j and self.z == z : \n",
    "                        s+= 'OO '\n",
    "                    else:\n",
    "                        l = ''\n",
    "                        if(self.map[i,j,z,0] == 0):\n",
    "                            l += '.'\n",
    "                        elif(self.map[i,j,z,0] == 1):\n",
    "                            l+='>'\n",
    "                        elif(self.map[i,j,z,0] == -1):\n",
    "                            l+='<'\n",
    "\n",
    "                        if(self.map[i,j,z,1] == 0):\n",
    "                            l += '.'\n",
    "                        elif(self.map[i,j,z,1] == 1):\n",
    "                            l+='v'\n",
    "                        elif(self.map[i,j,z,1] == -1):\n",
    "                            l+='^'\n",
    "                        \n",
    "                        s+= l+ \" \"\n",
    "                print(s)\n",
    "            print(\" \")\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalEnv(gym.Env):\n",
    "    def __init__(self, render_mode=None, s_x = 10 , s_y =10 , s_z = 3, obs_size = 5 ):\n",
    "        self.balloon = balloon3D(s_x,s_y,s_z,obs_size)\n",
    "        self.obs_size = obs_size\n",
    "\n",
    "        self._target_location = [s_x-1,s_y-1,s_z-1]\n",
    "\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                #\"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                #\"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"map\": spaces.Box(-1, 1, shape=(2*obs_size + 1,obs_size*2+1,s_z,2), dtype=int),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "    def reset(self):\n",
    "        self.evo_time = 100\n",
    "\n",
    "        self.reward = 0\n",
    "\n",
    "        self.reward_x, self.reward_y = 0,0\n",
    "\n",
    "        self.balloon.generate_map(0.35,0.5)\n",
    "\n",
    "        self.time = 0\n",
    "\n",
    "        self.balloon.reset(4,4,0)\n",
    "\n",
    "        self._agent_location = [0,0,0]\n",
    "\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.time += 1\n",
    "\n",
    "        if action == 0:\n",
    "            self.balloon.step()\n",
    "        if action == 1:\n",
    "            self.balloon.up()\n",
    "        if action == 2:\n",
    "            self.balloon.down()\n",
    "\n",
    "\n",
    "        \n",
    "        self._agent_location = np.array([self.balloon.x , self.balloon.y,self.balloon.z])\n",
    "        terminated  = ((self._agent_location[0]==self._target_location[0])and(self._agent_location[1]==self._target_location[1]))\n",
    "        \n",
    "        distx = min(((self._agent_location[0] - self._target_location[0])%self.balloon.width),(self._target_location[0] - self._agent_location[0])%self.balloon.width)\n",
    "        disty = min(((self._agent_location[1] - self._target_location[1])%self.balloon.length),(self._target_location[1] - self._agent_location[1])%self.balloon.length)\n",
    "\n",
    "\n",
    "\n",
    "        self.reward = -(np.sqrt((distx)**2 + (disty)**2))\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        trucated = (self.time == 50)\n",
    "\n",
    "        return observation, self.reward, terminated, trucated, {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return {\"map\" : self.balloon.get_winds()}\n",
    "\n",
    "    def render(self):\n",
    "        self.balloon.render()\n",
    "        \n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env = BalEnv(obs_size=2)\n",
    "wrapped_env = FlattenObservation(env)\n",
    "print(wrapped_env.action_space)\n",
    "\n",
    "training = DQN_Agent(wrapped_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.114220690017832"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[114], line 115\u001b[0m, in \u001b[0;36mDQN_Agent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    114\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Update the target Q-network in each 50 steps\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\monce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\monce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\monce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\monce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\monce\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    388\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    print(i)\n",
    "    training.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
