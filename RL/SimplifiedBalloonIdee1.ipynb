{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules en lien avec Gym\n",
    "import gymnasium as gym\n",
    "\n",
    "#Modules utiles\n",
    "import sys\n",
    "sys.path.append('/Users/mathiasperez/Documents/GitHub/PSC_Stratolia/Algo Naif')\n",
    "from parcours import parcours_a_Z, distance_destination, case, ventU_ventV\n",
    "import Node\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "#Modules en lien avec Stable_baseline3\n",
    "#IMPORT THE TYPE OF AGENT U WANT TO USE\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.bis Import the wind-datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"objet_wind_data_2020.pickle\", \"rb\") as f:\n",
    "    wind_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create the environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Balloon(gym.Env):\n",
    "    \"\"\"\n",
    "    ## Description\n",
    "    This environment is a complete representation of our problem.\n",
    "\n",
    "    The destination is always at variable coordinates (random opr not).\n",
    "    So are the starting coordinatesv (probably)\n",
    "\n",
    "\n",
    "    ## Action Space\n",
    "    There are three discrete actions available:\n",
    "    - 0: do nothing et let the wind lead your movement\n",
    "    - 1: go up in altitude (17 heights allowed)\n",
    "    - 2: go down in altitude (17 heights allowed)\n",
    "\n",
    "    ## Observation Space\n",
    "    The state a tuple consisting of different parts : \n",
    "        First you have the time accessible using the datetime module\n",
    "        Then 2 Boxes (simple array) for the longitude and the latitude\n",
    "        Finally one discrete part consisting of the 71 different heights\n",
    "\n",
    "    ## Rewards\n",
    "    After every step a reward is granted. The total reward of an episode is the\n",
    "    sum of the rewards for all the steps within that episode.\n",
    "\n",
    "    For each step, the reward:\n",
    "    (- is increased/decreased the closer/further the ballon is to the goal point.)\n",
    "    - is increased/decreased the slower/faster the lander is moving.\n",
    "    (- is increased/decreased the less/more energy from the battery is used in the round)\n",
    "\n",
    "    The episode receive an additional reward of +100 or -100 points for finding the goal or not respectively.\n",
    "\n",
    "\n",
    "\n",
    "    ## Starting State\n",
    "    The lander starts at a random poinf of the globe (time can be random as well)\n",
    "\n",
    "    ## Episode Termination\n",
    "    The episode finishes if:\n",
    "    1) the balloon gets too far away from the goal;\n",
    "    2) the balloon goes near enough of the goal\n",
    "\n",
    "    ## Arguments\n",
    "\n",
    "     * `goal_long` and 'goal_lat' for the position of our goal destination (can be fixed for specific results)\n",
    "     * 'initial_time' defines the starting time of our episode (=> consequence on the winds)\n",
    "\n",
    "    * `precision' defines how close the balloon must be to the target before being considered as having reached it.\n",
    "\n",
    "    * 'limite_eloignement' defines how far the ballon must be from the target to consider the episode to be a failure.\n",
    "\n",
    "    * `temps_exploration` defines how long we let the ballon follow the winds when action : 0 is considered\n",
    "\n",
    "    ## Version History\n",
    "    - v0: Initial version\n",
    "\n",
    "    ## Current bugs\n",
    "\n",
    "    ## Credits\n",
    "    CMOI\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Balloon, self).__init__()\n",
    "\n",
    "        self.goal_long = 2.65\n",
    "        self.goal_lat = 34.54\n",
    "\n",
    "        self.limite_eloignement = 10000 #en m\n",
    "        self.precision = 1000 #en m\n",
    "\n",
    "        self.initial_time = datetime(2021, 1, 1, 0, 0, 0)\n",
    "\n",
    "        self.temps_exploration = 3600\n",
    "        \n",
    "        # Define the action space\n",
    "        self.action_space = gym.spaces.Discrete(3)  \n",
    "        \"\"\"\n",
    "        Action 0 : You do nothing and let the balloon follow the wind for temps_exploration seconds\n",
    "        Action 1 : You push the balloon upwards (goes up in altitude) (instanneous action)\n",
    "        Action 2 : You push the balloon downwards (goes down in altitude) (instanneous action)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define the observation space\n",
    "        self.observation_space = gym.spaces.Tuple(( \n",
    "            gym.spaces.Box(low=datetime(2021, 1, 1, 0, 0, 0), high=datetime(2022, 1, 1, 0, 0, 0), shape=(1,), dtype=datetime), # time (in special format)\n",
    "            gym.spaces.Discrete(18), #Altitude (17 different positions : no need for more precisions)\n",
    "            gym.spaces.Box(low=0, high=180, shape=(1,), dtype=np.float32),  #Longitude\n",
    "            gym.spaces.Box(low=0, high=90, shape=(1,), dtype=np.float32)   #Latitude\n",
    "            ))\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the environment to its initial state\n",
    "        # Resetting the ballon to its initial position : 73 boulevard des maréchaux\n",
    "        #Ici on pourrait randomiser les goal et les positions de depart +temps\n",
    "        self.state = (np.array([self.initial_time], dtype=datetime), \n",
    "                      0, \n",
    "                      np.array([2.23], dtype=np.float32),\n",
    "                      np.array([50.23], dtype=np.float32)\n",
    "                      )\n",
    "        return self.state, {}\n",
    "    \n",
    "    def get_terminated(self, state):\n",
    "        long, lat = state[2], state[3]\n",
    "        if distance_destination((self.goal_long, self.goal_lat), long, lat) < self.limite_eloignement :\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        \"\"\"\n",
    "        - fonction qui donne récompense en fonction de la distance  a l'objectif par rapport a la distance initale\n",
    "        - ajoute un boolean si l'objectif a été atteint ou bien si on en est suffisamment proche\n",
    "        - d'autres trucs a faire surement\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Take a step in the environment based on the given action\n",
    "        if action == 0:\n",
    "            #You decide to follow the wind flow :\n",
    "            long, lat, pression = self.state[2], self.state[3], self.state[1]\n",
    "            secondes_depuis_debut = (self.state[0]-self.initial_time).total_seconds()\n",
    "            temps = secondes_depuis_debut//(6*3600), secondes_depuis_debut%(6*3600)\n",
    "            n = Node(long, lat,temps, pression, None) #longtiude, latitude, temps, ression, prev\n",
    "            terminated, n = parcours_a_Z(\n",
    "                                        (self.goal_long, self.goal_lat),\n",
    "                                        n,\n",
    "                                        temps_chgmt_pression = self.temps_exploration,\n",
    "                                        precision = self.precision, \n",
    "                                        tab_vent = wind_data)\n",
    "            done = terminated\n",
    "            new_date = self.initial_time + timedelta(seconds=n.temps[0]*(6*3600)+n.temps[1]) # A corriger\n",
    "            self.state = new_date, n.pression, n.long, n.lat\n",
    "\n",
    "        elif action == 1:\n",
    "            #You go up 1 case in altitude so u go down in pressure\n",
    "            self.state[1] -= 1\n",
    "        else :\n",
    "            #You go down one case in altitudeso u go up in pressure\n",
    "            self.state[1] += 1\n",
    "    \n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = get_reward(self.state)\n",
    "        \n",
    "        # Check if the episode has ended\n",
    "        if not done:\n",
    "            done = get_terminated(self.state)\n",
    "        \n",
    "        # Check if the episode has been truncated\n",
    "        truncated = False\n",
    "        \n",
    "        # Return the observation, reward, done flag, truncated flag, and info\n",
    "        return self.state, reward, done, truncated, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        # Render the environment\n",
    "        print(\"State:\", self.state)\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
